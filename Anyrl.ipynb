{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anyrl.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eqHJZlYAPK0_"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyHaski/DL_AtariRainbow/blob/master/Anyrl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm7YwXSflftO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/45/Alien_pixel.png\" height=\"100\"/>\n",
        "</div>\n",
        "<h1>Deep Reinforcement Learning: Rainbow in Atari Simulator</h1>\n",
        "\n",
        "\n",
        "This notebook is supposed to show how to train an Atari game with the Rainbow Agent for the Deep Learning Seminar at the University of Offenburg.\n",
        "\n",
        "An brief explanation of the agent is provided as well as the code snippets for hyperparameter tuning and training the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GzKRIRNUhzh",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jpZ2iynNO-7",
        "colab_type": "text"
      },
      "source": [
        "The framework used is called [Anyrl-py](https://github.com/unixpickle/anyrl-py). It is a open-source framework for Reinforcement Learning implementing different algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsT1hn__UWop",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install Packages\n",
        "!pip install anyrl\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqHJZlYAPK0_",
        "colab_type": "text"
      },
      "source": [
        "# Explanation: Rainbow Agent\n",
        "The Rainbow Agent combines different variants and extensions of DQN (Deep Q-learning Network). To get an understanding of the agent a quick explanation of Q-learning, DQN itself and its extentions are in the following text sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrqHsWt3QiZD",
        "colab_type": "text"
      },
      "source": [
        "## Q-learning\n",
        "\n",
        "Q-learning is a category of model-free temporal-difference learning. It tries to find a policy to take the best action at a specific state to get a maximum future reward. It's called Q-learning because it calculates the quality of a action-state value pair. It takes the reward of the current step and add the estimated discounted max future reward.\n",
        "The discount factor determines how important the values of the timestep after the curent are.\n",
        "The function looks as follows:\n",
        "</br>\n",
        "</br>\n",
        "![alt text](https://cdn-media-1.freecodecamp.org/images/TnN7ys7VGKoDszzv3WDnr5H8txOj3KKQ0G8o)\n",
        "<br>\n",
        "Source: https://cdn-media-1.freecodecamp.org/images/TnN7ys7VGKoDszzv3WDnr5H8txOj3KKQ0G8o\n",
        "</br>\n",
        "</br>\n",
        "When the problem is small all the state-action pairs can be saved in a so called Q-table, which basically functions as a cheat sheet to determine the best next action. The bigger the problem, the bigger the table gets. This lead to combining deep neural network with the Q-Function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWqnQRUknkZK",
        "colab_type": "text"
      },
      "source": [
        "##DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzd6y-GWQkiK",
        "colab_type": "text"
      },
      "source": [
        "DQN represents the Q-function of Q-learning with a deep neural network. \n",
        "The details of the DQN was featured in the [nature paper](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf), which also includes this image.\n",
        "</br></br>\n",
        "![alt text](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnature14236/MediaObjects/41586_2015_Article_BFnature14236_Fig1_HTML.jpg)\n",
        "</br></br>\n",
        "\n",
        "The network itself takes 4 input images of the game, so the movement of the game is visible. The images go through 3 convolutional layers and 2 fully connected layers at the end. The final output of the network are the actions available in a atari game, 18 in total.\n",
        "\n",
        "In DQN three important things were introdruced to the Q-learning Algorithm.\n",
        "\n",
        "><h3> 1. ϵ-greedy strategy</h3>\n",
        "\n",
        "At the beginning of training the way actions are chosen is explorativ, because in the beginning it's not clear which action will yield the best reward. So to get a lot of knowledge of the different state-action Q-Values, exploration is favored.\n",
        "</br>\n",
        "\n",
        "After some time training the strategy of choosing actions is changed to exploitation. The actions with the best rewards are more often choosen. The combination of the strategies enables a more stable learning.\n",
        "\n",
        "> <h3> 2. Experience replay buffer</h3> \n",
        "\n",
        "The transistion are saved into a buffer during training. In the Rainbow Agent the last million transisitions are saved. Random mini batches are taken from the buffer to be the input data for the next training loop. This leads to no forgotten experiences and reduces the correlations between experiences.\n",
        "\n",
        "> <h3> 3. Online and Target Network</h3> \n",
        "\n",
        "The current Q-value and the future max reward which - is the goal of the training - are calculated with the same parameters in normal Q-learning. This leads to correlations between the two values. Is the current Q-value updated, the future reward is updated with the same parameters leading to a moving target reward. This is bad as the target reward will never be reached.\n",
        "<br>\n",
        "To fix this two networks are introduced: an online and target network. The online network is responsible for calculating the current Q-values and updating the parameters during training while the target network only updates periodically through copying the online network. \n",
        "<br>\n",
        "\n",
        "The Q-function in DQN looks like the following, which determines the Q-loss:\n",
        "\n",
        ">$R_{t+1} + \\gamma_{t+1}max q_{\\bar{\\theta}}(S_{t+1},a')-q_{{\\theta}}(S_{t},A_{t}))^2$\n",
        "\n",
        "---\n",
        "\n",
        "### Extentions and Variants of DQN\n",
        "\n",
        "### Double DQN\n",
        "\n",
        "In Double DQN the action selection and evaluation of the action is split. This leads to more stable learning and counters the overestimation bias of normal DQN, which stems from the maximization step. The Q-loss is changed to represent this:\n",
        "\n",
        ">$R_{t+1} + \\gamma_{t+1}q_{\\bar{\\theta}}(S_{t+1},argmax (S_{t+1},a'))-q_{{\\theta}}(S_{t},A_{t}))^2$\n",
        "\n",
        "---\n",
        "\n",
        "### Multi-Step DQN\n",
        "\n",
        "Normally DQN only calculates for one step one reward. In multi-step DQN *n* steps are calculated and *n* rewards are being returned. This leads to significantly faster learning:\n",
        "\n",
        ">$R_{t}^{(n)} + \\gamma_{t}^{(n)}max q_{\\bar{\\theta}}(S_{t+n},a')-q_{{\\theta}}(S_{t},A_{t}))^2$\n",
        "\n",
        "---\n",
        "\n",
        "### Prioritized Replay Buffer\n",
        "\n",
        "The samples of the replay buffer are normally randomly sampled. In the case of the prioritized replay buffer, the transistions with high Q-loss are prioritized as these transistions are the ones, from which the most can be learned.\n",
        "\n",
        "---\n",
        "\n",
        "### Dueling Architecture\n",
        "\n",
        "There are two streams in the dueling architecture. One stream calculates the value of a state $V(s)$, while the other stream calculates the advantage of an action over other actions at a specific state $A(s,a)$. This leads to not having to calculate all actions for a state which isn't valuable as any action in that state wouldn't impact the game.\n",
        "The two streams are later aggregated through a special aggregation layer to get an estimate of $Q(s,a)$. When aggregating the two streams a simple addition of the two values wouldn't suffice as $V(s)$ and $A(s,a)$ woulnd't be identifiable in the backpropagation. The problem can be avoided through subtracting the average advantage of all actions of the specific state, .\n",
        "\n",
        "---\n",
        "\n",
        "### Distributional RL\n",
        "\n",
        "Normally the average estimated Q-value is used as target​. But using the average Q-values are not that accurate as the Q-value can be diverse in different situations. Instead of using the average Q-value distributional reinforcement learning is used to learn the distribution of Q-values. The resulting loss is the *Kullback-Leibler* divergence.\n",
        "\n",
        "---\n",
        "\n",
        "### Noisy Nets\n",
        "\n",
        "Noisy Nets can be used to replace the ϵ-greedy strategy for the action selection. It adds noise to all linear layers which leads to changes in the exploration rate automatically. During training itself the agent can learn to ignore the noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYbavM7BZ2H8",
        "colab_type": "text"
      },
      "source": [
        "## Rainbow Agent\n",
        "\n",
        "The Rainbow Agent combines all 6 variants and extensions of DQN, creating a new state-of-the-art agent which plays atari games better then all other subcombinations of DQN. The agent \n",
        "replaces the 1-step distributional loss with a multi-step variant​ and combines this with Double DQN. For the replay buffer a proportional prioritized replay is used which prioritizes transistions by the *KL* loss. The dueling network is adapted for use with the return of the distribution learning. At last all linear layers are replaced with a noisy net equivalent.\n",
        "<br>\n",
        "The following graph show the improvments of the score in atari games.\n",
        "\n",
        "![alt text](https://media.arxiv-vanity.com/render-output/1731808/x1.png)\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kru-aYTU8sps",
        "colab_type": "text"
      },
      "source": [
        "#Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-GCyt6c8-ss",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "When using the Rainbow Agent a lot of hyperparameters need to be set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EMKo2kZ55nR",
        "colab_type": "text"
      },
      "source": [
        "## General Parameters\n",
        "\n",
        "This sections is for general settings like the save path and which game to train.\n",
        "<br>\n",
        "\n",
        "*  **RUN_RESTORE** - the run number to restore\n",
        "*  **RUN_NUM** - stand for subrun of one big run if the training sessions want to be split.\n",
        "*  **path** - path to save data to\n",
        "*  **restore** - to load saved model and transition\n",
        "*  **game** - game to train\n",
        "*  **workers** - how many workers should work in the env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kswz0Ca1Bi20",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Organisation settings\n",
        "\n",
        "RUN='1' #@param {type: \"string\"}\n",
        "RUN_RESTORE='1_1' #@param {type: \"string\"}\n",
        "RUN_NUM='1_1' #@param {type: \"string\"}\n",
        "path='/tmp/anyrl_rainbow' #@param {type: \"string\"}\n",
        "restore=False #@param {type: \"boolean\"}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84-jkbYk3Vnx",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Training settings\n",
        "\n",
        "game='SpaceInvaders' #@param\n",
        "workers=8 #@param {type: \"integer\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmNV4Xj854aR",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "The next sections is for setting the hyperparameters. The parameters are split into groups of the specific extension of the Rainbow algorithm for a clearer overview.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZrmWbtvcHsq",
        "colab_type": "text"
      },
      "source": [
        "The default values of the rainbow agent are:\n",
        "```\n",
        "lr=6.25e-5 \n",
        "num_steps=6000000 \n",
        "train_interval=4 \n",
        "batch_size=32 \n",
        "target_interval=8192 \n",
        "num_atoms=51 \n",
        "v_min=-10 \n",
        "v_max=10 \n",
        "n_step=3 \n",
        "min_buffer_size=20000 \n",
        "buffer_size=1000000 \n",
        "replay_epsilon=0.1 \n",
        "replay_alpha=0.5 \n",
        "replay_beta=0.4 \n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcFHV5e_dlbp",
        "colab_type": "text"
      },
      "source": [
        "### Training Parameters\n",
        "The Rainbow Agent uses the AdamOptimizer. \n",
        "* **lr**  - learning rate\n",
        "* **num_steps** - amount of steps to learn\n",
        "* **train_interval** - impacts the learning speed. Basically every 4 steps the networkt learns. \n",
        "* **batch_size** - input dataset; the amount to sample from the transitions\n",
        "* **target_interval** -  is the period between update of the online and target network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAjBuKRc5Yd_",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "lr=6.25e-5 #@param {type:\"number\"}\n",
        "num_steps=6000000 #@param {type: \"integer\"}\n",
        "train_interval=4 #@param {type: \"integer\"}\n",
        "batch_size=32 #@param {type: \"integer\"}\n",
        "target_interval=8192 #@param {type: \"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWIGQpmLe30q",
        "colab_type": "text"
      },
      "source": [
        "### Distributional Parameters\n",
        "*  **num_atoms** - number of atoms in the distribution\n",
        "*  **v_min** - lowest value of the distribution\n",
        "*  **v_max** - highest value of the distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8xm96Yy5VnW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "\n",
        "num_atoms=51 #@param {type: \"integer\"}\n",
        "v_min=-10 #@param {type: \"integer\"}\n",
        "v_max=10 #@param {type: \"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMlifxgjfqDt",
        "colab_type": "text"
      },
      "source": [
        "### Multi-Step Parameters\n",
        "\n",
        "\n",
        "*   **n_step** - amount of steps to take in multi-step\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skj9h2eB5UEJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "n_step=3 #@param {type: \"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBty-zR4diIM",
        "colab_type": "text"
      },
      "source": [
        "###Replay Buffer Parameters\n",
        "It's supposed to start learning after 80000 frames passed and the transistions saved. As the input of the network are four frames at once, this means one step equals four frames. Consequently after 20000 steps training can start. The buffer_size is normally 1000000 but as a size this big would be diffuclt to save it, was reduced.\n",
        "\n",
        "*   **min_buffer_size** - min size to start learning\n",
        "*   **buffer_size** - size of buffer saving transistions\n",
        "*   **replay_epsilon** - value added to every error term\n",
        "*   **replay_alpha** - controlling the temperature. Higher values result in more prioritization. A value of 0 yields uniform prioritization\n",
        "*   **replay_beta** - controlling amount of importance sampling.A value of 1 yields unbiased sampling. A value of 0 yields no importance sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM7eCOHr4was",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "min_buffer_size=20000 #@param {type: \"integer\"}\n",
        "buffer_size=500000 #@param {type: \"integer\"}\n",
        "replay_epsilon=0.1 #@param {type:\"number\"}\n",
        "replay_alpha=0.5 #@param {type:\"number\"}\n",
        "replay_beta=0.4 #@param {type:\"number\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBbH049m27SH",
        "colab_type": "text"
      },
      "source": [
        "# Modified Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUb0WpT9K_4n",
        "colab_type": "text"
      },
      "source": [
        "The replay buffer of the framework was modified to save the amount of transisitions in the replay buffer, which was specified in the parameters of the hyperparameter section. This way the training can be continued at a later time.\n",
        "The original buffer can be found at:\n",
        "https://github.com/unixpickle/anyrl-py/blob/master/anyrl/rollouts/replay.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGFWQ9KT27u9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Modified Replay Buffer Code\n",
        "\"\"\"\n",
        "Various replay buffer implementations.\n",
        "\"\"\"\n",
        "\n",
        "from math import sqrt\n",
        "import random\n",
        "import pickle\n",
        "from anyrl.rollouts import ReplayBuffer\n",
        "import numpy as np\n",
        "\n",
        "class ModifiedPrioritizedReplayBuffer(ReplayBuffer):\n",
        "    \"\"\"\n",
        "    A prioritized replay buffer with loss-proportional\n",
        "    sampling.\n",
        "    Weights passed to add_sample() and update_weights()\n",
        "    are assumed to be error terms (e.g. the absolute TD\n",
        "    error).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity, alpha, beta, first_max=1, epsilon=0):\n",
        "        \"\"\"\n",
        "        Create a prioritized replay buffer.\n",
        "        The beta parameter can be any object that has\n",
        "        support for the float() built-in.\n",
        "        This way, you can use a TFScheduleValue.\n",
        "        Args:\n",
        "          capacity: the maximum number of transitions to\n",
        "            store in the buffer.\n",
        "          alpha: an exponent controlling the temperature.\n",
        "            Higher values result in more prioritization.\n",
        "            A value of 0 yields uniform prioritization.\n",
        "          beta: an exponent controlling the amount of\n",
        "            importance sampling. A value of 1 yields\n",
        "            unbiased sampling. A value of 0 yields no\n",
        "            importance sampling.\n",
        "          first_max: the initial weight for new samples\n",
        "            when no init_weight is specified and the\n",
        "            buffer is completely empty.\n",
        "          epsilon: a value which is added to every error\n",
        "            term before the error term is used.\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.transitions = []\n",
        "        self.errors = FloatBuffer(capacity)\n",
        "        self._max_weight_arg = first_max\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return len(self.transitions)\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        indices, probs = self.errors.sample(num_samples)\n",
        "        beta = float(self.beta)\n",
        "        importance_weights = np.power(probs * self.size, -beta)\n",
        "        importance_weights /= np.power(self.errors.min() / self.errors.sum() * self.size, -beta)\n",
        "        samples = []\n",
        "        for i, weight in zip(indices, importance_weights):\n",
        "            sample = self.transitions[i].copy()\n",
        "            sample['weight'] = weight\n",
        "            sample['id'] = i\n",
        "            samples.append(sample)\n",
        "        return samples\n",
        "\n",
        "    def add_sample(self, sample, init_weight=None):\n",
        "        \"\"\"\n",
        "        Add a sample to the buffer.\n",
        "        When new samples are added without an explicit\n",
        "        initial weight, the maximum weight argument ever\n",
        "        seen is used. When the buffer is empty, first_max\n",
        "        is used.\n",
        "        \"\"\"\n",
        "        self.transitions.append(sample)\n",
        "        if init_weight is None:\n",
        "            self.errors.append(self._process_weight(self._max_weight_arg))\n",
        "        else:\n",
        "            self.errors.append(self._process_weight(init_weight))\n",
        "        while len(self.transitions) > self.capacity:\n",
        "            del self.transitions[0]\n",
        "\n",
        "    def update_weights(self, samples, new_weights):\n",
        "        for sample, weight in zip(samples, new_weights):\n",
        "            self.errors.set_value(sample['id'], self._process_weight(weight))\n",
        "\n",
        "    def _process_weight(self, weight):\n",
        "        self._max_weight_arg = max(self._max_weight_arg, weight)\n",
        "        return (weight + self.epsilon) ** self.alpha\n",
        "\n",
        "    \"\"\"\n",
        "    The two functions were added to save the transitions\n",
        "    \"\"\"\n",
        "    #saving replay buffer to pickle file\n",
        "    def save_samples(self):\n",
        "        print(\"saving transitions: \",len(self.transitions))\n",
        "        with open(path+'/'+RUN+'/transitions_'+RUN_NUM+'.p','wb') as handle:\n",
        "            pickle.dump(self.transitions,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    #loading replay buffer from pickle file\n",
        "     #have to add sample by sample due to populating the errors into the FloatBuffer (self.errors)\n",
        "    def load_samples(self):\n",
        "        temp_trans = pickle.load(open(path+'/'+RUN+\"/transitions_\"+RUN_RESTORE+\".p\",\"rb\"))\n",
        "        for i in temp_trans:\n",
        "            self.add_sample(i)\n",
        "        print(\"loaded transitions: \",len(self.transitions))\n",
        "\n",
        "class FloatBuffer:\n",
        "    \"\"\"A ring-buffer of floating point values.\"\"\"\n",
        "\n",
        "    def __init__(self, capacity, dtype='float64'):\n",
        "        self._capacity = capacity\n",
        "        self._start = 0\n",
        "        self._used = 0\n",
        "        self._buffer = np.zeros((capacity,), dtype=dtype)\n",
        "        self._bin_size = int(sqrt(capacity))\n",
        "        num_bins = capacity // self._bin_size\n",
        "        if num_bins * self._bin_size < capacity:\n",
        "            num_bins += 1\n",
        "        self._bin_sums = np.zeros((num_bins,), dtype=dtype)\n",
        "        self._min = 0\n",
        "\n",
        "    def append(self, value):\n",
        "        \"\"\"\n",
        "        Add a value to the end of the buffer.\n",
        "        If the buffer is full, the first value is removed.\n",
        "        \"\"\"\n",
        "        idx = (self._start + self._used) % self._capacity\n",
        "        if self._used < self._capacity:\n",
        "            self._used += 1\n",
        "        else:\n",
        "            self._start = (self._start + 1) % self._capacity\n",
        "        self._set_idx(idx, value)\n",
        "\n",
        "    def sample(self, num_values):\n",
        "        \"\"\"\n",
        "        Sample indices in proportion to their value.\n",
        "        Returns:\n",
        "          A tuple (indices, probs)\n",
        "        \"\"\"\n",
        "        assert self._used >= num_values\n",
        "        res = []\n",
        "        probs = []\n",
        "        bin_probs = self._bin_sums / np.sum(self._bin_sums)\n",
        "        while len(res) < num_values:\n",
        "            bin_idx = np.random.choice(len(self._bin_sums), p=bin_probs)\n",
        "            bin_values = self._bin(bin_idx)\n",
        "            sub_probs = bin_values / np.sum(bin_values)\n",
        "            sub_idx = np.random.choice(len(bin_values), p=sub_probs)\n",
        "            idx = bin_idx * self._bin_size + sub_idx\n",
        "            res.append(idx)\n",
        "            probs.append(bin_probs[bin_idx] * sub_probs[sub_idx])\n",
        "        return (np.array(list(res)) - self._start) % self._capacity, np.array(probs)\n",
        "\n",
        "    def set_value(self, idx, value):\n",
        "        \"\"\"Set the value at the given index.\"\"\"\n",
        "        idx = (idx + self._start) % self._capacity\n",
        "        self._set_idx(idx, value)\n",
        "\n",
        "    def min(self):\n",
        "        \"\"\"Get the minimum value in the buffer.\"\"\"\n",
        "        return self._min\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Get the sum of the values in the buffer.\"\"\"\n",
        "        return np.sum(self._bin_sums)\n",
        "\n",
        "    def _set_idx(self, idx, value):\n",
        "        assert not np.isnan(value)\n",
        "        assert value > 0\n",
        "        needs_recompute = False\n",
        "        if self._min == self._buffer[idx]:\n",
        "            needs_recompute = True\n",
        "        elif value < self._min:\n",
        "            self._min = value\n",
        "        bin_idx = idx // self._bin_size\n",
        "        self._buffer[idx] = value\n",
        "        self._bin_sums[bin_idx] = np.sum(self._bin(bin_idx))\n",
        "        if needs_recompute:\n",
        "            self._recompute_min()\n",
        "\n",
        "    def _bin(self, bin_idx):\n",
        "        if bin_idx == len(self._bin_sums) - 1:\n",
        "            return self._buffer[self._bin_size * bin_idx:]\n",
        "        return self._buffer[self._bin_size * bin_idx:self._bin_size * (bin_idx + 1)]\n",
        "\n",
        "    def _recompute_min(self):\n",
        "        if self._used < self._capacity:\n",
        "            self._min = np.min(self._buffer[:self._used])\n",
        "        else:\n",
        "            self._min = np.min(self._buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rmQG7AZ28a5",
        "colab_type": "text"
      },
      "source": [
        "In the training loop a code section to record the values of different parameters where added. Specifically the discount, weight, losses and rewards while training.\n",
        "The original code can be found at:\n",
        "https://github.com/unixpickle/anyrl-py/blob/master/anyrl/algos/dqn.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q28xoaCVBFG_",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Modified DQN Code\n",
        "%tensorflow_version 1.x\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class DQN:\n",
        "    \"\"\"\n",
        "    Train TFQNetwork models using Q-learning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, online_net, target_net, discount=0.99):\n",
        "        \"\"\"\n",
        "        Create a Q-learning session.\n",
        "        Args:\n",
        "          online_net: the online TFQNetwork.\n",
        "          target_net: the target TFQNetwork.\n",
        "          discount: the per-step discount factor.\n",
        "        \"\"\"\n",
        "        self.online_net = online_net\n",
        "        self.target_net = target_net\n",
        "        self.discount = discount\n",
        "\n",
        "        obs_shape = (None,) + online_net.obs_vectorizer.out_shape\n",
        "        self.obses_ph = tf.placeholder(online_net.input_dtype, shape=obs_shape)\n",
        "        self.actions_ph = tf.placeholder(tf.int32, shape=(None,))\n",
        "        self.rews_ph = tf.placeholder(tf.float32, shape=(None,))\n",
        "        self.new_obses_ph = tf.placeholder(online_net.input_dtype, shape=obs_shape)\n",
        "        self.terminals_ph = tf.placeholder(tf.bool, shape=(None,))\n",
        "        self.discounts_ph = tf.placeholder(tf.float32, shape=(None,))\n",
        "        self.weights_ph = tf.placeholder(tf.float32, shape=(None,))\n",
        "\n",
        "        losses = online_net.transition_loss(target_net, self.obses_ph, self.actions_ph,\n",
        "                                            self.rews_ph, self.new_obses_ph, self.terminals_ph,\n",
        "                                            self.discounts_ph)\n",
        "        self.losses = self.weights_ph * losses\n",
        "        self.loss = tf.reduce_mean(self.losses)\n",
        "\n",
        "        assigns = []\n",
        "        for dst, src in zip(target_net.variables, online_net.variables):\n",
        "            assigns.append(tf.assign(dst, src))\n",
        "        self.update_target = tf.group(*assigns)\n",
        "\n",
        "    def feed_dict(self, transitions):\n",
        "        \"\"\"\n",
        "        Generate a feed_dict that feeds the batch of\n",
        "        transitions to the DQN loss terms.\n",
        "        Args:\n",
        "          transition: a sequence of transition dicts, as\n",
        "            defined in anyrl.rollouts.ReplayBuffer.\n",
        "        Returns:\n",
        "          A dict which can be fed to tf.Session.run().\n",
        "        \"\"\"\n",
        "        obs_vect = self.online_net.obs_vectorizer\n",
        "        res = {\n",
        "            self.obses_ph: obs_vect.to_vecs([t['obs'] for t in transitions]),\n",
        "            self.actions_ph: [t['model_outs']['actions'][0] for t in transitions],\n",
        "            self.rews_ph: [self._discounted_rewards(t['rewards']) for t in transitions],\n",
        "            self.terminals_ph: [t['new_obs'] is None for t in transitions],\n",
        "            self.discounts_ph: [(self.discount ** len(t['rewards'])) for t in transitions],\n",
        "            self.weights_ph: [t['weight'] for t in transitions]\n",
        "        }\n",
        "        new_obses = []\n",
        "        for trans in transitions:\n",
        "            if trans['new_obs'] is None:\n",
        "                new_obses.append(trans['obs'])\n",
        "            else:\n",
        "                new_obses.append(trans['new_obs'])\n",
        "        res[self.new_obses_ph] = obs_vect.to_vecs(new_obses)\n",
        "        return res\n",
        "\n",
        "    def optimize(self, learning_rate=6.25e-5, epsilon=1.5e-4, **adam_kwargs):\n",
        "        \"\"\"\n",
        "        Create a TF Op that optimizes the objective.\n",
        "        Args:\n",
        "          learning_rate: the Adam learning rate.\n",
        "          epsilon: the Adam epsilon.\n",
        "        \"\"\"\n",
        "        optim = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon, **adam_kwargs)\n",
        "        return optim.minimize(self.loss)\n",
        "\n",
        "    def train(self,\n",
        "              num_steps,\n",
        "              player,\n",
        "              replay_buffer,\n",
        "              optimize_op,\n",
        "              train_interval=1,\n",
        "              target_interval=8192,\n",
        "              batch_size=32,\n",
        "              min_buffer_size=20000,\n",
        "              tf_schedules=(),\n",
        "              handle_ep=lambda steps, rew: None,\n",
        "              timeout=None):\n",
        "        \"\"\"\n",
        "        Run an automated training loop.\n",
        "        This is meant to provide a convenient way to run a\n",
        "        standard training loop without any modifications.\n",
        "        You may get more flexibility by writing your own\n",
        "        training loop.\n",
        "        Args:\n",
        "          num_steps: the number of timesteps to run.\n",
        "          player: the Player for gathering experience.\n",
        "          replay_buffer: the ReplayBuffer for experience.\n",
        "          optimize_op: a TF Op to optimize the model.\n",
        "          train_interval: timesteps per training step.\n",
        "          target_interval: number of timesteps between\n",
        "            target network updates.\n",
        "          batch_size: the size of experience mini-batches.\n",
        "          min_buffer_size: minimum replay buffer size\n",
        "            before training is performed.\n",
        "          tf_schedules: a sequence of TFSchedules that are\n",
        "            updated with the number of steps taken.\n",
        "          handle_ep: called with information about every\n",
        "            completed episode.\n",
        "          timeout: if set, this is a number of seconds\n",
        "            after which the training loop should exit.\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        sess = self.online_net.session\n",
        "        \n",
        "        \"\"\"\n",
        "        This section was added to record the values\n",
        "        \"\"\"\n",
        "        tnrewsph = tf.summary.scalar(name='rews_ph', tensor=tf.reduce_mean(self.rews_ph))\n",
        "        tndiscountsph = tf.summary.scalar(name='discounts_ph', tensor=tf.reduce_mean(self.discounts_ph))\n",
        "        tnweightsph = tf.summary.scalar(name='weights_ph', tensor=tf.reduce_mean(self.weights_ph))\n",
        "        tnlosses = tf.summary.scalar(name='losses', tensor=tf.reduce_mean(self.losses))\n",
        "        merge = tf.summary.merge([tnrewsph,tndiscountsph,tnweightsph,tnlosses])\n",
        "        train_writer = tf.summary.FileWriter( path+'/'+RUN+'/logs/'+ RUN_NUM +'/train', sess.graph)\n",
        "        \n",
        "        \n",
        "        sess.run(self.update_target)\n",
        "        steps_taken = 0\n",
        "        next_target_update = target_interval\n",
        "        next_train_step = train_interval\n",
        "        start_time = time.time()\n",
        "\n",
        "        if restore:\n",
        "          replay_buffer.load_samples()\n",
        "\n",
        "        while steps_taken < num_steps:\n",
        "            if timeout is not None and time.time() - start_time > timeout:\n",
        "                return\n",
        "            transitions = player.play()\n",
        "            for trans in transitions:\n",
        "                if trans['is_last']:\n",
        "                    handle_ep(trans['episode_step'] + 1, trans['total_reward'], trans['episode_id'])\n",
        "                replay_buffer.add_sample(trans)\n",
        "                steps_taken += 1\n",
        "                for sched in tf_schedules:\n",
        "                    sched.add_time(sess, 1)\n",
        "                if replay_buffer.size >= min_buffer_size and steps_taken >= next_train_step:\n",
        "                    next_train_step = steps_taken + train_interval\n",
        "                    batch = replay_buffer.sample(batch_size)\n",
        "                    \n",
        "                    _, losses, summary = sess.run((optimize_op, self.losses, merge),\n",
        "                                         feed_dict=self.feed_dict(batch))\n",
        "                    \n",
        "                    train_writer.add_summary(summary, steps_taken)\n",
        "                    replay_buffer.update_weights(batch, losses)\n",
        "                    \n",
        "                if steps_taken >= next_target_update:\n",
        "                    next_target_update = steps_taken + target_interval\n",
        "                    sess.run(self.update_target)\n",
        "                if (steps_taken % 100000 ==0):\n",
        "                   replay_buffer.save_samples()\n",
        "        replay_buffer.save_samples()\n",
        "\n",
        "    def _discounted_rewards(self, rews):\n",
        "        res = 0\n",
        "        for i, rew in enumerate(rews):\n",
        "            res += rew * (self.discount ** i)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQDjfSEC26UV",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K2gkzIdSCbX",
        "colab_type": "text"
      },
      "source": [
        "The training loop was modified to allow recoding of videos while training as well as saving parameters and rewards of each epiode in the training process. An ouput is printed every 10 episodes with the episode id and the mean of the last 10 episodes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "_NF0GLaDM4bL",
        "colab": {}
      },
      "source": [
        "#@title DQN Train\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "from gym.wrappers import Monitor\n",
        "from functools import partial\n",
        "from anyrl.envs import batched_gym_env,BatchedGymEnv\n",
        "from anyrl.envs.wrappers import BatchedFrameStack, DownsampleEnv, GrayscaleEnv\n",
        "from anyrl.models import rainbow_models\n",
        "from anyrl.rollouts import BatchedPlayer, NStepPlayer\n",
        "from anyrl.spaces import gym_space_vectorizer\n",
        "from anyrl.utils import tf_state\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "REWARD_HISTORY = 10\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, path+'/'+RUN+'/videos/'+RUN_NUM, force=True )\n",
        "  return env\n",
        "\n",
        "def make_env():\n",
        "    \"\"\"\n",
        "    Create an environment with some standard wrappers.\n",
        "    \"\"\"\n",
        "    env = wrap_env(gym.make(game+'-v0'))\n",
        "    env = GrayscaleEnv(DownsampleEnv(env, 2))\n",
        "    return env\n",
        "\n",
        "def main():\n",
        "\n",
        "    env = batched_gym_env([partial(make_env)]* workers)\n",
        "    env = BatchedFrameStack(env, num_images=4, concat=False)\n",
        "\n",
        "    checkpoint_dir = os.path.join(os.getcwd(), path+'/'+RUN+'/checkpoints/'+RUN_NUM)\n",
        "    results_dir = os.path.join(os.getcwd(), path+'/'+RUN+'/results/', time.strftime(\"%d-%m-%Y_%H-%M-%S__\")+ RUN_NUM)\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "    summary_writer = tf.summary.FileWriter(results_dir)\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True # pylint: disable=E1101\n",
        "    \n",
        "    \n",
        "    with tf.Session(config=config) as sess:  \n",
        "        \n",
        "        dqn=DQN(*rainbow_models(sess,\n",
        "                              env.action_space.n,\n",
        "                              gym_space_vectorizer(env.observation_space),\n",
        "                              num_atoms=num_atoms,\n",
        "                              min_val=v_min,\n",
        "                              max_val=v_max))\n",
        "\n",
        "        player = NStepPlayer(BatchedPlayer(env, dqn.online_net), n_step)\n",
        "        optimize = dqn.optimize(learning_rate=lr)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        if(restore):\n",
        "          tf_state.load_vars(sess,path+\"/\"+RUN+\"/anyrlModel_\"+RUN_RESTORE)\n",
        "\n",
        "        reward_hist = []\n",
        "        total_steps = 0\n",
        "        save_model_steps=100000\n",
        "        episodes=0\n",
        "\n",
        "        def _handle_ep(steps, rew, id):\n",
        "            nonlocal total_steps\n",
        "            nonlocal episodes\n",
        "            nonlocal save_model_steps\n",
        "            total_steps += steps\n",
        "            episodes += 1\n",
        "            reward_hist.append(rew)\n",
        "\n",
        "            summary_reward = tf.Summary()\n",
        "            summary_reward.value.add(tag='global/reward', simple_value=rew)\n",
        "            summary_writer.add_summary(summary_reward, global_step=total_steps)\n",
        "\n",
        "            if len(reward_hist) == REWARD_HISTORY:\n",
        "              print('ID: %d | %d steps | %f mean' % (id,total_steps, sum(reward_hist) / len(reward_hist)))\n",
        "            \n",
        "              summary_meanreward = tf.Summary()\n",
        "              summary_meanreward.value.add(tag='global/mean_reward', simple_value=sum(reward_hist) / len(reward_hist))\n",
        "              summary_writer.add_summary(summary_meanreward, global_step=total_steps)\n",
        "\n",
        "              reward_hist.clear()\n",
        "            if(total_steps>= save_model_steps):\n",
        "              save_model_steps +=100000\n",
        "              print('save model')\n",
        "              tf_state.save_vars(sess,path+\"/\"+RUN+\"/anyrlModel_\"+RUN_NUM)\n",
        "                 \n",
        "        \n",
        "        dqn.train(num_steps=num_steps, \n",
        "                  player=player,\n",
        "                  replay_buffer=ModifiedPrioritizedReplayBuffer(buffer_size, replay_alpha, replay_beta, epsilon=replay_epsilon),\n",
        "                  optimize_op=optimize,\n",
        "                  train_interval=train_interval,\n",
        "                  target_interval=target_interval,\n",
        "                  batch_size=batch_size,\n",
        "                  min_buffer_size=min_buffer_size,\n",
        "                  handle_ep=_handle_ep)\n",
        "        \n",
        "        print('save model')\n",
        "        tf_state.save_vars(sess,path+\"/\"+RUN+\"/anyrlModel_\"+RUN_NUM)\n",
        "    env.close()\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "      main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v-GqeiDcFLK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Starting Tensorboard for the logs\n",
        "\n",
        "LOG_DIR = path+'/'+RUN+'/logs/'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHkRd1370TVi",
        "colab_type": "text"
      },
      "source": [
        "Sometimes the ngrok link won't open. If that happens try running:\n",
        "\n",
        "```\n",
        "!tensorboard --logdir=path+'/'+RUN+'/logs/' \n",
        "```\n",
        "\n",
        "And click on the previous link again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOtnAxity_lE",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Show the newest video of current run\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import glob\n",
        "import os\n",
        "\n",
        "list_of_files = glob.glob(path+'/'+RUN+'/videos/'+RUN_NUM+'/*.mp4') # * means all if need specific format then *.csv\n",
        "latest_file = max(list_of_files, key=os.path.getctime)\n",
        "print(latest_file)\n",
        "\n",
        "mp4 = open(latest_file,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U22tyDOWKxck",
        "colab_type": "text"
      },
      "source": [
        "# Links\n",
        "\n",
        "\n",
        "**Q-learning and DQN**\n",
        "* https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/\n",
        "* https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8/\n",
        "* https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/\n",
        "* https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4\n",
        "\n",
        "* https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
        "\n",
        "**Rainbow Agent**\n",
        "\n",
        "* https://arxiv.org/pdf/1710.02298.pdf\n",
        "* https://medium.com/intelligentunit/conquering-openai-retro-contest-2-demystifying-rainbow-baseline-9d8dd258e74b"
      ]
    }
  ]
}