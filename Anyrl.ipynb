{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anyrl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyHaski/DL_AtariRainbow/blob/master/Anyrl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GzKRIRNUhzh",
        "colab_type": "code",
        "outputId": "10496337-6f57-45ed-feba-a063d823f69a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsT1hn__UWop",
        "colab_type": "code",
        "outputId": "9b48d7df-2173-4750-c336-5e40ff909edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "source": [
        "!pip install anyrl "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting anyrl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/07/8924a62b35dfb681d88c148de055059ccc8adf1759551281aae733bdfb64/anyrl-0.12.23-py3-none-any.whl (89kB)\n",
            "\r\u001b[K     |███▋                            | 10kB 31.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from anyrl) (1.17.4)\n",
            "Requirement already satisfied: cloudpickle>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from anyrl) (1.2.2)\n",
            "Collecting pandas<0.24.0,>=0.20.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 7.1MB/s \n",
            "\u001b[?25hCollecting gym<0.11.0,>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/04/70d4901b7105082c9742acd64728342f6da7cd471572fd0660a73f9cfe27/gym-0.10.11.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas<0.24.0,>=0.20.0->anyrl) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas<0.24.0,>=0.20.0->anyrl) (2.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym<0.11.0,>=0.9.6->anyrl) (1.3.3)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.11.0,>=0.9.6->anyrl) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.11.0,>=0.9.6->anyrl) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.11.0,>=0.9.6->anyrl) (1.3.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<0.11.0,>=0.9.6->anyrl) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<0.11.0,>=0.9.6->anyrl) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<0.11.0,>=0.9.6->anyrl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<0.11.0,>=0.9.6->anyrl) (2019.11.28)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym<0.11.0,>=0.9.6->anyrl) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.11-cp36-none-any.whl size=1588314 sha256=e74c1159a24a6b01b9a1bf62c4d2924560b963fef773148d92c0bc59c97b0ba0\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/eb/1f/22c4124f3c64943aa0646daf4612b1c1f00f27d89b81304ebd\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.25.0; python_version >= \"3.0\", but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pandas, gym, anyrl\n",
            "  Found existing installation: pandas 0.25.3\n",
            "    Uninstalling pandas-0.25.3:\n",
            "      Successfully uninstalled pandas-0.25.3\n",
            "  Found existing installation: gym 0.15.4\n",
            "    Uninstalling gym-0.15.4:\n",
            "      Successfully uninstalled gym-0.15.4\n",
            "Successfully installed anyrl-0.12.23 gym-0.10.11 pandas-0.23.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kswz0Ca1Bi20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RUN='3'\n",
        "RUN_NUM='3_1'\n",
        "\n",
        "\"\"\"\n",
        "Hyperparameters\n",
        "\n",
        "\"\"\"\n",
        "lr=6.25e-5\n",
        "lr_epsilon=1.5e-4\n",
        "num_atoms=51\n",
        "v_min=-10\n",
        "v_max=10\n",
        "noisy_sigma=0.5\n",
        "n_step=3\n",
        "#dueling=True\n",
        "min_buffer_size=20000\n",
        "num_steps=2000000\n",
        "buffer_size=500000\n",
        "#workers=8\n",
        "target_interval=8192\n",
        "batch_size=32\n",
        "replay_epsilon=0.0\n",
        "replay_alpha=0.5\n",
        "replay_beta=0.4\n",
        "game='SpaceInvaders'\n",
        "restore=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGFWQ9KT27u9",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Modified Replay Buffer Code\n",
        "\"\"\"\n",
        "Various replay buffer implementations.\n",
        "\"\"\"\n",
        "\n",
        "from math import sqrt\n",
        "import random\n",
        "import pickle\n",
        "from anyrl.rollouts import ReplayBuffer\n",
        "import numpy as np\n",
        "\n",
        "class ModifiedPrioritizedReplayBuffer(ReplayBuffer):\n",
        "    \"\"\"\n",
        "    A prioritized replay buffer with loss-proportional\n",
        "    sampling.\n",
        "    Weights passed to add_sample() and update_weights()\n",
        "    are assumed to be error terms (e.g. the absolute TD\n",
        "    error).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity, alpha, beta, first_max=1, epsilon=0):\n",
        "        \"\"\"\n",
        "        Create a prioritized replay buffer.\n",
        "        The beta parameter can be any object that has\n",
        "        support for the float() built-in.\n",
        "        This way, you can use a TFScheduleValue.\n",
        "        Args:\n",
        "          capacity: the maximum number of transitions to\n",
        "            store in the buffer.\n",
        "          alpha: an exponent controlling the temperature.\n",
        "            Higher values result in more prioritization.\n",
        "            A value of 0 yields uniform prioritization.\n",
        "          beta: an exponent controlling the amount of\n",
        "            importance sampling. A value of 1 yields\n",
        "            unbiased sampling. A value of 0 yields no\n",
        "            importance sampling.\n",
        "          first_max: the initial weight for new samples\n",
        "            when no init_weight is specified and the\n",
        "            buffer is completely empty.\n",
        "          epsilon: a value which is added to every error\n",
        "            term before the error term is used.\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.transitions = []\n",
        "        self.errors = FloatBuffer(capacity)\n",
        "        self._max_weight_arg = first_max\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return len(self.transitions)\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        indices, probs = self.errors.sample(num_samples)\n",
        "        beta = float(self.beta)\n",
        "        importance_weights = np.power(probs * self.size, -beta)\n",
        "        importance_weights /= np.power(self.errors.min() / self.errors.sum() * self.size, -beta)\n",
        "        samples = []\n",
        "        for i, weight in zip(indices, importance_weights):\n",
        "            sample = self.transitions[i].copy()\n",
        "            sample['weight'] = weight\n",
        "            sample['id'] = i\n",
        "            samples.append(sample)\n",
        "        return samples\n",
        "\n",
        "    def add_sample(self, sample, init_weight=None):\n",
        "        \"\"\"\n",
        "        Add a sample to the buffer.\n",
        "        When new samples are added without an explicit\n",
        "        initial weight, the maximum weight argument ever\n",
        "        seen is used. When the buffer is empty, first_max\n",
        "        is used.\n",
        "        \"\"\"\n",
        "        self.transitions.append(sample)\n",
        "        if init_weight is None:\n",
        "            self.errors.append(self._process_weight(self._max_weight_arg))\n",
        "        else:\n",
        "            self.errors.append(self._process_weight(init_weight))\n",
        "        while len(self.transitions) > self.capacity:\n",
        "            del self.transitions[0]\n",
        "\n",
        "    def update_weights(self, samples, new_weights):\n",
        "        for sample, weight in zip(samples, new_weights):\n",
        "            self.errors.set_value(sample['id'], self._process_weight(weight))\n",
        "\n",
        "    def _process_weight(self, weight):\n",
        "        self._max_weight_arg = max(self._max_weight_arg, weight)\n",
        "        return (weight + self.epsilon) ** self.alpha\n",
        "\n",
        "    #saving replay buffer to pickle file\n",
        "    def save_samples(self):\n",
        "        print(\"saving transitions: \",len(self.transitions))\n",
        "        with open('/content/drive/My Drive/AN/transitions'+RUN_NUM+'.p','wb') as handle:\n",
        "            pickle.dump(self.transitions,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    #loading replay buffer from pickle file\n",
        "        #have to add sample by sample due to populating the errors into the FloatBuffer (self.errors)\n",
        "    def load_samples(self):\n",
        "        temp_trans = pickle.load(open(\"/content/drive/My Drive/AN/transitions\"+RUN_NUM+\".p\",\"rb\"))\n",
        "        for i in temp_trans:\n",
        "            self.add_sample(i)\n",
        "        print(\"loaded transitions: \",len(self.transitions))\n",
        "\n",
        "class FloatBuffer:\n",
        "    \"\"\"A ring-buffer of floating point values.\"\"\"\n",
        "\n",
        "    def __init__(self, capacity, dtype='float64'):\n",
        "        self._capacity = capacity\n",
        "        self._start = 0\n",
        "        self._used = 0\n",
        "        self._buffer = np.zeros((capacity,), dtype=dtype)\n",
        "        self._bin_size = int(sqrt(capacity))\n",
        "        num_bins = capacity // self._bin_size\n",
        "        if num_bins * self._bin_size < capacity:\n",
        "            num_bins += 1\n",
        "        self._bin_sums = np.zeros((num_bins,), dtype=dtype)\n",
        "        self._min = 0\n",
        "\n",
        "    def append(self, value):\n",
        "        \"\"\"\n",
        "        Add a value to the end of the buffer.\n",
        "        If the buffer is full, the first value is removed.\n",
        "        \"\"\"\n",
        "        idx = (self._start + self._used) % self._capacity\n",
        "        if self._used < self._capacity:\n",
        "            self._used += 1\n",
        "        else:\n",
        "            self._start = (self._start + 1) % self._capacity\n",
        "        self._set_idx(idx, value)\n",
        "\n",
        "    def sample(self, num_values):\n",
        "        \"\"\"\n",
        "        Sample indices in proportion to their value.\n",
        "        Returns:\n",
        "          A tuple (indices, probs)\n",
        "        \"\"\"\n",
        "        assert self._used >= num_values\n",
        "        res = []\n",
        "        probs = []\n",
        "        bin_probs = self._bin_sums / np.sum(self._bin_sums)\n",
        "        while len(res) < num_values:\n",
        "            bin_idx = np.random.choice(len(self._bin_sums), p=bin_probs)\n",
        "            bin_values = self._bin(bin_idx)\n",
        "            sub_probs = bin_values / np.sum(bin_values)\n",
        "            sub_idx = np.random.choice(len(bin_values), p=sub_probs)\n",
        "            idx = bin_idx * self._bin_size + sub_idx\n",
        "            res.append(idx)\n",
        "            probs.append(bin_probs[bin_idx] * sub_probs[sub_idx])\n",
        "        return (np.array(list(res)) - self._start) % self._capacity, np.array(probs)\n",
        "\n",
        "    def set_value(self, idx, value):\n",
        "        \"\"\"Set the value at the given index.\"\"\"\n",
        "        idx = (idx + self._start) % self._capacity\n",
        "        self._set_idx(idx, value)\n",
        "\n",
        "    def min(self):\n",
        "        \"\"\"Get the minimum value in the buffer.\"\"\"\n",
        "        return self._min\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Get the sum of the values in the buffer.\"\"\"\n",
        "        return np.sum(self._bin_sums)\n",
        "\n",
        "    def _set_idx(self, idx, value):\n",
        "        assert not np.isnan(value)\n",
        "        assert value > 0\n",
        "        needs_recompute = False\n",
        "        if self._min == self._buffer[idx]:\n",
        "            needs_recompute = True\n",
        "        elif value < self._min:\n",
        "            self._min = value\n",
        "        bin_idx = idx // self._bin_size\n",
        "        self._buffer[idx] = value\n",
        "        self._bin_sums[bin_idx] = np.sum(self._bin(bin_idx))\n",
        "        if needs_recompute:\n",
        "            self._recompute_min()\n",
        "\n",
        "    def _bin(self, bin_idx):\n",
        "        if bin_idx == len(self._bin_sums) - 1:\n",
        "            return self._buffer[self._bin_size * bin_idx:]\n",
        "        return self._buffer[self._bin_size * bin_idx:self._bin_size * (bin_idx + 1)]\n",
        "\n",
        "    def _recompute_min(self):\n",
        "        if self._used < self._capacity:\n",
        "            self._min = np.min(self._buffer[:self._used])\n",
        "        else:\n",
        "            self._min = np.min(self._buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q28xoaCVBFG_",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Modified DQN Code\n",
        "%tensorflow_version 1.x\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class DQN:\n",
        "    \"\"\"\n",
        "    Train TFQNetwork models using Q-learning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, online_net, target_net, discount=0.99):\n",
        "        \"\"\"\n",
        "        Create a Q-learning session.\n",
        "        Args:\n",
        "          online_net: the online TFQNetwork.\n",
        "          target_net: the target TFQNetwork.\n",
        "          discount: the per-step discount factor.\n",
        "        \"\"\"\n",
        "        self.online_net = online_net\n",
        "        self.target_net = target_net\n",
        "        self.discount = discount\n",
        "\n",
        "        obs_shape = (None,) + online_net.obs_vectorizer.out_shape\n",
        "        self.obses_ph = tf.placeholder(online_net.input_dtype, shape=obs_shape)\n",
        "        self.actions_ph = tf.placeholder(tf.int32, shape=(None,))\n",
        "        self.rews_ph = tf.placeholder(tf.float32, shape=(None,))\n",
        "        self.new_obses_ph = tf.placeholder(online_net.input_dtype, shape=obs_shape)\n",
        "        self.terminals_ph = tf.placeholder(tf.bool, shape=(None,))\n",
        "        self.discounts_ph = tf.placeholder(tf.float32, shape=(None,))\n",
        "        self.weights_ph = tf.placeholder(tf.float32, shape=(None,))\n",
        "\n",
        "        losses = online_net.transition_loss(target_net, self.obses_ph, self.actions_ph,\n",
        "                                            self.rews_ph, self.new_obses_ph, self.terminals_ph,\n",
        "                                            self.discounts_ph)\n",
        "        self.losses = self.weights_ph * losses\n",
        "        self.loss = tf.reduce_mean(self.losses)\n",
        "\n",
        "        assigns = []\n",
        "        for dst, src in zip(target_net.variables, online_net.variables):\n",
        "            assigns.append(tf.assign(dst, src))\n",
        "        self.update_target = tf.group(*assigns)\n",
        "\n",
        "    def feed_dict(self, transitions):\n",
        "        \"\"\"\n",
        "        Generate a feed_dict that feeds the batch of\n",
        "        transitions to the DQN loss terms.\n",
        "        Args:\n",
        "          transition: a sequence of transition dicts, as\n",
        "            defined in anyrl.rollouts.ReplayBuffer.\n",
        "        Returns:\n",
        "          A dict which can be fed to tf.Session.run().\n",
        "        \"\"\"\n",
        "        obs_vect = self.online_net.obs_vectorizer\n",
        "        res = {\n",
        "            self.obses_ph: obs_vect.to_vecs([t['obs'] for t in transitions]),\n",
        "            self.actions_ph: [t['model_outs']['actions'][0] for t in transitions],\n",
        "            self.rews_ph: [self._discounted_rewards(t['rewards']) for t in transitions],\n",
        "            self.terminals_ph: [t['new_obs'] is None for t in transitions],\n",
        "            self.discounts_ph: [(self.discount ** len(t['rewards'])) for t in transitions],\n",
        "            self.weights_ph: [t['weight'] for t in transitions]\n",
        "        }\n",
        "        new_obses = []\n",
        "        for trans in transitions:\n",
        "            if trans['new_obs'] is None:\n",
        "                new_obses.append(trans['obs'])\n",
        "            else:\n",
        "                new_obses.append(trans['new_obs'])\n",
        "        res[self.new_obses_ph] = obs_vect.to_vecs(new_obses)\n",
        "        return res\n",
        "\n",
        "    def optimize(self, learning_rate=6.25e-5, epsilon=1.5e-4, **adam_kwargs):\n",
        "        \"\"\"\n",
        "        Create a TF Op that optimizes the objective.\n",
        "        Args:\n",
        "          learning_rate: the Adam learning rate.\n",
        "          epsilon: the Adam epsilon.\n",
        "        \"\"\"\n",
        "        optim = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon, **adam_kwargs)\n",
        "        return optim.minimize(self.loss)\n",
        "\n",
        "    def train(self,\n",
        "              num_steps,\n",
        "              player,\n",
        "              replay_buffer,\n",
        "              optimize_op,\n",
        "              train_interval=1,\n",
        "              target_interval=8192,\n",
        "              batch_size=32,\n",
        "              min_buffer_size=20000,\n",
        "              tf_schedules=(),\n",
        "              handle_ep=lambda steps, rew: None,\n",
        "              timeout=None):\n",
        "        \"\"\"\n",
        "        Run an automated training loop.\n",
        "        This is meant to provide a convenient way to run a\n",
        "        standard training loop without any modifications.\n",
        "        You may get more flexibility by writing your own\n",
        "        training loop.\n",
        "        Args:\n",
        "          num_steps: the number of timesteps to run.\n",
        "          player: the Player for gathering experience.\n",
        "          replay_buffer: the ReplayBuffer for experience.\n",
        "          optimize_op: a TF Op to optimize the model.\n",
        "          train_interval: timesteps per training step.\n",
        "          target_interval: number of timesteps between\n",
        "            target network updates.\n",
        "          batch_size: the size of experience mini-batches.\n",
        "          min_buffer_size: minimum replay buffer size\n",
        "            before training is performed.\n",
        "          tf_schedules: a sequence of TFSchedules that are\n",
        "            updated with the number of steps taken.\n",
        "          handle_ep: called with information about every\n",
        "            completed episode.\n",
        "          timeout: if set, this is a number of seconds\n",
        "            after which the training loop should exit.\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        sess = self.online_net.session\n",
        "        \n",
        "        tnrewsph = tf.summary.scalar(name='rews_ph', tensor=tf.reduce_mean(self.rews_ph))\n",
        "        tndiscountsph = tf.summary.scalar(name='discounts_ph', tensor=tf.reduce_mean(self.discounts_ph))\n",
        "        tnweightsph = tf.summary.scalar(name='weights_ph', tensor=tf.reduce_mean(self.weights_ph))\n",
        "        tnlosses = tf.summary.scalar(name='losses', tensor=tf.reduce_mean(self.losses))\n",
        "\n",
        "        merge = tf.summary.merge([tnrewsph,tndiscountsph,tnweightsph,tnlosses])\n",
        "        train_writer = tf.summary.FileWriter( '/content/drive/My Drive/AN/logs/'+ RUN_NUM +'/train', sess.graph)\n",
        "        \n",
        "        sess.run(self.update_target)\n",
        "        steps_taken = 0\n",
        "        next_target_update = target_interval\n",
        "        next_train_step = train_interval\n",
        "        start_time = time.time()\n",
        "\n",
        "        if restore:\n",
        "          replay_buffer.load_samples()\n",
        "\n",
        "        while steps_taken < num_steps:\n",
        "            if timeout is not None and time.time() - start_time > timeout:\n",
        "                return\n",
        "            transitions = player.play()\n",
        "            for trans in transitions:\n",
        "                if trans['is_last']:\n",
        "                    handle_ep(trans['episode_step'] + 1, trans['total_reward'], trans['episode_id'])\n",
        "                replay_buffer.add_sample(trans)\n",
        "                steps_taken += 1\n",
        "                for sched in tf_schedules:\n",
        "                    sched.add_time(sess, 1)\n",
        "                if replay_buffer.size >= min_buffer_size and steps_taken >= next_train_step:\n",
        "                    next_train_step = steps_taken + train_interval\n",
        "                    batch = replay_buffer.sample(batch_size)\n",
        "                    \n",
        "                    #_, losses = sess.run((optimize_op, self.losses),\n",
        "                    #                     feed_dict=self.feed_dict(batch))\n",
        "                    _, losses, summary = sess.run((optimize_op, self.losses, merge),\n",
        "                                         feed_dict=self.feed_dict(batch))\n",
        "                    \n",
        "                    train_writer.add_summary(summary, steps_taken)\n",
        "                    replay_buffer.update_weights(batch, losses)\n",
        "                    \n",
        "                if steps_taken >= next_target_update:\n",
        "                    next_target_update = steps_taken + target_interval\n",
        "                    sess.run(self.update_target)\n",
        "                if (steps_taken % 100000 ==0):\n",
        "                   replay_buffer.save_samples()\n",
        "\n",
        "    def _discounted_rewards(self, rews):\n",
        "        res = 0\n",
        "        for i, rew in enumerate(rews):\n",
        "            res += rew * (self.discount ** i)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37Krqkiff8Sc",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title para agrs don't work \n",
        "import argparse\n",
        "\n",
        "def _parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--lr', help='Adam learning rate', type=float, default=6.25e-5)\n",
        "    parser.add_argument('--lr-epsilon', help='Adam epsilon', type=float, default=1.5e-4)\n",
        "    parser.add_argument('--num-atoms', help='Number of distribution atoms', type=int, default=51)\n",
        "    parser.add_argument('--v-min', help='Minimum atom value', type=int, default=-10)\n",
        "    parser.add_argument('--v-max', help='Maximum atom value', type=int, default=10)\n",
        "    parser.add_argument('--noisy-sigma', help='initial Noisy Net noise', type=float, default=0.5)\n",
        "    parser.add_argument('--n-step', help='Number of multi-step', type=int, default=3)\n",
        "    #parser.add_argument('--dueling', help='Set dueling architecture', type=bool, default=True)\n",
        "    parser.add_argument('--min-buffer-size', help='replay buffer size before training',\n",
        "                        type=int, default=20000)\n",
        "    parser.add_argument('--num-steps', help='Number of max steps to train', type=int, default=2000000)\n",
        "    parser.add_argument('--buffer-size', help='replay buffer size', type=int, default=1000000)\n",
        "   #parser.add_argument('--workers', help='number of parallel envs', type=int, default=8)\n",
        "    parser.add_argument('--target-interval', help='training iters per log', type=int, default=8192)\n",
        "    parser.add_argument('--batch-size', help='SGD batch size', type=int, default=32)\n",
        "    parser.add_argument('--replay-epsilon', help='initial epsilon', type=float, default=0.0)\n",
        "    parser.add_argument('--replay-alpha', help='Replay alpha', type=float, default=0.5)\n",
        "    parser.add_argument('--replay-beta', help='Replay beta', type=float, default=0.4)\n",
        "    parser.add_argument('game', help='game name', default='SpaceInvaders')\n",
        "    parser.add_argument('--restore', help='load from checkpoint', action='store_true')\n",
        "    return parser.parse_args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vq_lTHZV3iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afc4j-0zMTC2",
        "colab_type": "code",
        "outputId": "09b9c50d-b8de-46b4-efbb-3b7dfc366ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!/usr/bin/env python\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "from gym.wrappers import Monitor\n",
        "from functools import partial\n",
        "from anyrl.envs import batched_gym_env\n",
        "from anyrl.envs.wrappers import BatchedFrameStack, DownsampleEnv, GrayscaleEnv\n",
        "from anyrl.models import rainbow_models\n",
        "from anyrl.rollouts import BatchedPlayer, NStepPlayer\n",
        "from anyrl.spaces import gym_space_vectorizer\n",
        "from anyrl.utils import tf_state\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "REWARD_HISTORY = 10\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, '/content/drive/My Drive/videoAnyrl/'+RUN_NUM, force=True )\n",
        "  return env\n",
        "\n",
        "def make_env():\n",
        "    \"\"\"\n",
        "    Create an environment with some standard wrappers.\n",
        "    \"\"\"\n",
        "    #env = retro.make(game='SpaceInvaders-Atari2600', record=True)\n",
        "    env = wrap_env(gym.make(game+'-v0'))\n",
        "    return GrayscaleEnv(DownsampleEnv(env, 2))\n",
        "\n",
        "def main():\n",
        "    \n",
        "    env = batched_gym_env([partial(make_env)])\n",
        "\n",
        "    env = BatchedFrameStack(env, num_images=4, concat=False)\n",
        "\n",
        "    checkpoint_dir = os.path.join(os.getcwd(), 'drive/My Drive/ANresults')\n",
        "    results_dir = os.path.join(os.getcwd(), 'drive/My Drive/ANresults', time.strftime(\"%d-%m-%Y_%H-%M-%S\"))\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "    summary_writer = tf.summary.FileWriter(results_dir)\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True # pylint: disable=E1101\n",
        "    \n",
        "    \n",
        "    with tf.Session(config=config) as sess:  \n",
        "        \n",
        "        dqn=DQN(*rainbow_models(sess,\n",
        "                              env.action_space.n,\n",
        "                              gym_space_vectorizer(env.observation_space),\n",
        "                              num_atoms=num_atoms,\n",
        "                              min_val=v_min,\n",
        "                              max_val=v_max))\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        #if restore:\n",
        "         #   latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "         #   #saver = tf.train.import_meta_graph(checkpoint_dir)\n",
        "         #   if latest_checkpoint:\n",
        "         #       print(\"Loading model checkpoint {} ...\\n\".format(latest_checkpoint))\n",
        "         #       saver.restore(sess, latest_checkpoint)\n",
        "         #   else:\n",
        "         #     print(\"Checkpoint not found\")     \n",
        "\n",
        "\n",
        "        player = NStepPlayer(BatchedPlayer(env, dqn.online_net), n_step)\n",
        "        optimize = dqn.optimize(lr, lr_epsilon)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        if(restore):\n",
        "          tf_state.load_vars(sess,'/content/drive/My Drive/AN/anyrlM'+RUN_NUM)\n",
        "\n",
        "        reward_hist = []\n",
        "        total_steps = 0\n",
        "        save_model_steps=100000\n",
        "        episodes=0\n",
        "\n",
        "        def _handle_ep(steps, rew, id):\n",
        "            nonlocal total_steps\n",
        "            nonlocal episodes\n",
        "            nonlocal save_model_steps\n",
        "            total_steps += steps\n",
        "            episodes += 1\n",
        "            reward_hist.append(rew)\n",
        "\n",
        "            summary_reward = tf.Summary()\n",
        "            summary_reward.value.add(tag='global/reward', simple_value=rew)\n",
        "            summary_writer.add_summary(summary_reward, global_step=total_steps)\n",
        "\n",
        "            if len(reward_hist) == REWARD_HISTORY:\n",
        "              print('ID: %d | %d steps | %f mean' % (id,total_steps, sum(reward_hist) / len(reward_hist)))\n",
        "            \n",
        "              summary_meanreward = tf.Summary()\n",
        "              summary_meanreward.value.add(tag='global/mean_reward', simple_value=sum(reward_hist) / len(reward_hist))\n",
        "              summary_writer.add_summary(summary_meanreward, global_step=total_steps)\n",
        "\n",
        "              reward_hist.clear()\n",
        "            if(total_steps>= save_model_steps):\n",
        "              save_model_steps +=100000\n",
        "              print('save model')\n",
        "              saver.save(sess=sess, save_path=checkpoint_dir + '/model', global_step=total_steps)\n",
        "              tf_state.save_vars(sess,'/content/drive/My Drive/AN/anyrlM'+RUN_NUM)\n",
        "                 \n",
        "        \n",
        "        dqn.train(num_steps=num_steps, \n",
        "                  player=player,\n",
        "                  replay_buffer=ModifiedPrioritizedReplayBuffer(buffer_size, replay_alpha, replay_beta, epsilon=replay_epsilon),\n",
        "                  optimize_op=optimize,\n",
        "                  train_interval=1,\n",
        "                  target_interval=target_interval,\n",
        "                  batch_size=batch_size,\n",
        "                  min_buffer_size=min_buffer_size,\n",
        "                  timeout=3600*10,\n",
        "                  handle_ep=_handle_ep)\n",
        "        \n",
        "    env.close()\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "      main()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:78: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:79: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:80: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/util.py:80: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_scalar.py:247: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_scalar.py:260: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:113: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:273: The name tf.ceil is deprecated. Please use tf.math.ceil instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:289: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "ID: 9 | 7046 steps | 81.000000 mean\n",
            "ID: 19 | 14111 steps | 95.000000 mean\n",
            "ID: 29 | 21881 steps | 147.500000 mean\n",
            "ID: 39 | 31031 steps | 192.000000 mean\n",
            "ID: 49 | 39708 steps | 260.000000 mean\n",
            "ID: 59 | 47573 steps | 244.000000 mean\n",
            "ID: 69 | 55944 steps | 214.500000 mean\n",
            "ID: 79 | 65295 steps | 263.500000 mean\n",
            "ID: 89 | 72478 steps | 201.500000 mean\n",
            "ID: 99 | 78947 steps | 171.500000 mean\n",
            "ID: 109 | 85679 steps | 171.000000 mean\n",
            "ID: 119 | 94768 steps | 337.000000 mean\n",
            "saving transitions:  100000\n",
            "save model\n",
            "ID: 129 | 101020 steps | 164.500000 mean\n",
            "ID: 139 | 108215 steps | 171.500000 mean\n",
            "ID: 149 | 115721 steps | 219.500000 mean\n",
            "ID: 159 | 123039 steps | 244.500000 mean\n",
            "ID: 169 | 130561 steps | 239.000000 mean\n",
            "ID: 179 | 139176 steps | 262.500000 mean\n",
            "ID: 189 | 145840 steps | 198.500000 mean\n",
            "ID: 199 | 154243 steps | 262.500000 mean\n",
            "ID: 209 | 161166 steps | 209.000000 mean\n",
            "ID: 219 | 169300 steps | 260.500000 mean\n",
            "ID: 229 | 176605 steps | 159.500000 mean\n",
            "ID: 239 | 183408 steps | 200.500000 mean\n",
            "ID: 249 | 189852 steps | 173.000000 mean\n",
            "ID: 259 | 196917 steps | 200.000000 mean\n",
            "saving transitions:  200000\n",
            "save model\n",
            "ID: 269 | 204902 steps | 215.500000 mean\n",
            "ID: 279 | 213108 steps | 215.000000 mean\n",
            "ID: 289 | 221639 steps | 262.500000 mean\n",
            "ID: 299 | 228934 steps | 174.000000 mean\n",
            "ID: 309 | 237016 steps | 283.500000 mean\n",
            "ID: 319 | 245481 steps | 261.000000 mean\n",
            "ID: 329 | 254353 steps | 274.500000 mean\n",
            "ID: 339 | 262278 steps | 222.500000 mean\n",
            "ID: 349 | 270333 steps | 235.500000 mean\n",
            "ID: 359 | 278359 steps | 223.500000 mean\n",
            "ID: 369 | 285383 steps | 184.500000 mean\n",
            "ID: 379 | 293460 steps | 248.500000 mean\n",
            "ID: 389 | 299974 steps | 144.000000 mean\n",
            "saving transitions:  300000\n",
            "save model\n",
            "ID: 399 | 306259 steps | 156.500000 mean\n",
            "ID: 409 | 313521 steps | 205.000000 mean\n",
            "ID: 419 | 322184 steps | 244.000000 mean\n",
            "ID: 429 | 330889 steps | 273.500000 mean\n",
            "ID: 439 | 337952 steps | 190.500000 mean\n",
            "ID: 449 | 345738 steps | 220.000000 mean\n",
            "ID: 459 | 354801 steps | 260.500000 mean\n",
            "ID: 469 | 361838 steps | 202.500000 mean\n",
            "ID: 479 | 368681 steps | 207.000000 mean\n",
            "ID: 489 | 377631 steps | 259.500000 mean\n",
            "ID: 499 | 385113 steps | 251.000000 mean\n",
            "ID: 509 | 393590 steps | 250.500000 mean\n",
            "saving transitions:  400000\n",
            "save model\n",
            "ID: 519 | 402486 steps | 302.000000 mean\n",
            "ID: 529 | 410812 steps | 284.000000 mean\n",
            "ID: 539 | 418388 steps | 266.500000 mean\n",
            "ID: 549 | 426770 steps | 270.500000 mean\n",
            "ID: 559 | 432998 steps | 168.000000 mean\n",
            "ID: 569 | 440600 steps | 190.500000 mean\n",
            "ID: 579 | 446786 steps | 188.500000 mean\n",
            "ID: 589 | 453761 steps | 200.500000 mean\n",
            "ID: 599 | 461233 steps | 220.000000 mean\n",
            "ID: 609 | 468064 steps | 207.500000 mean\n",
            "ID: 619 | 474654 steps | 181.000000 mean\n",
            "ID: 629 | 482647 steps | 282.500000 mean\n",
            "ID: 639 | 491279 steps | 270.500000 mean\n",
            "ID: 649 | 499068 steps | 243.000000 mean\n",
            "saving transitions:  500000\n",
            "save model\n",
            "ID: 659 | 507878 steps | 301.000000 mean\n",
            "ID: 669 | 515723 steps | 238.000000 mean\n",
            "ID: 679 | 522932 steps | 234.000000 mean\n",
            "ID: 689 | 531111 steps | 238.000000 mean\n",
            "ID: 699 | 538635 steps | 231.000000 mean\n",
            "ID: 709 | 547653 steps | 302.000000 mean\n",
            "ID: 719 | 555055 steps | 204.500000 mean\n",
            "ID: 729 | 562941 steps | 294.000000 mean\n",
            "ID: 739 | 571164 steps | 275.500000 mean\n",
            "ID: 749 | 578209 steps | 218.000000 mean\n",
            "ID: 759 | 585657 steps | 289.500000 mean\n",
            "ID: 769 | 593278 steps | 247.000000 mean\n",
            "saving transitions:  500000\n",
            "save model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "ID: 779 | 600996 steps | 289.000000 mean\n",
            "ID: 789 | 609070 steps | 232.000000 mean\n",
            "ID: 799 | 618419 steps | 315.000000 mean\n",
            "ID: 809 | 624772 steps | 179.500000 mean\n",
            "ID: 819 | 632320 steps | 276.500000 mean\n",
            "ID: 829 | 640435 steps | 294.500000 mean\n",
            "ID: 839 | 647756 steps | 217.500000 mean\n",
            "ID: 849 | 655611 steps | 230.500000 mean\n",
            "ID: 859 | 663370 steps | 258.000000 mean\n",
            "ID: 869 | 671988 steps | 271.000000 mean\n",
            "ID: 879 | 681368 steps | 292.000000 mean\n",
            "ID: 889 | 688782 steps | 224.000000 mean\n",
            "ID: 899 | 695715 steps | 215.000000 mean\n",
            "saving transitions:  500000\n",
            "save model\n",
            "ID: 909 | 704871 steps | 305.000000 mean\n",
            "ID: 919 | 712318 steps | 232.500000 mean\n",
            "ID: 929 | 720967 steps | 305.000000 mean\n",
            "ID: 939 | 727906 steps | 195.000000 mean\n",
            "ID: 949 | 735611 steps | 251.000000 mean\n",
            "ID: 959 | 743398 steps | 217.500000 mean\n",
            "ID: 969 | 751299 steps | 279.500000 mean\n",
            "ID: 979 | 758896 steps | 222.000000 mean\n",
            "ID: 989 | 766337 steps | 192.500000 mean\n",
            "ID: 999 | 775092 steps | 287.000000 mean\n",
            "ID: 1009 | 782410 steps | 216.500000 mean\n",
            "ID: 1019 | 789299 steps | 203.500000 mean\n",
            "ID: 1029 | 797316 steps | 296.500000 mean\n",
            "saving transitions:  500000\n",
            "save model\n",
            "ID: 1039 | 805591 steps | 230.500000 mean\n",
            "ID: 1049 | 813691 steps | 243.000000 mean\n",
            "ID: 1059 | 821914 steps | 248.000000 mean\n",
            "ID: 1069 | 829529 steps | 234.000000 mean\n",
            "ID: 1079 | 836854 steps | 242.000000 mean\n",
            "ID: 1089 | 843423 steps | 195.000000 mean\n",
            "ID: 1099 | 850454 steps | 202.000000 mean\n",
            "ID: 1109 | 859022 steps | 286.000000 mean\n",
            "ID: 1119 | 864823 steps | 166.000000 mean\n",
            "ID: 1129 | 871315 steps | 185.000000 mean\n",
            "ID: 1139 | 879123 steps | 245.000000 mean\n",
            "ID: 1149 | 886612 steps | 238.500000 mean\n",
            "ID: 1159 | 894470 steps | 259.000000 mean\n",
            "saving transitions:  500000\n",
            "save model\n",
            "ID: 1169 | 902582 steps | 290.000000 mean\n",
            "ID: 1179 | 909641 steps | 218.500000 mean\n",
            "ID: 1189 | 918735 steps | 336.500000 mean\n",
            "ID: 1199 | 926501 steps | 278.500000 mean\n",
            "ID: 1209 | 934851 steps | 308.500000 mean\n",
            "ID: 1219 | 942946 steps | 303.500000 mean\n",
            "ID: 1229 | 950210 steps | 251.500000 mean\n",
            "ID: 1239 | 958426 steps | 313.500000 mean\n",
            "ID: 1249 | 966877 steps | 243.000000 mean\n",
            "ID: 1259 | 974485 steps | 247.000000 mean\n",
            "ID: 1269 | 982519 steps | 299.000000 mean\n",
            "ID: 1279 | 991910 steps | 388.500000 mean\n",
            "saving transitions:  500000\n",
            "ID: 1289 | 1001009 steps | 369.500000 mean\n",
            "save model\n",
            "ID: 1299 | 1007663 steps | 215.500000 mean\n",
            "ID: 1309 | 1014974 steps | 262.500000 mean\n",
            "ID: 1319 | 1022138 steps | 261.500000 mean\n",
            "ID: 1329 | 1027990 steps | 154.500000 mean\n",
            "ID: 1339 | 1036568 steps | 294.000000 mean\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:89: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3b7313ce4401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m       \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-3b7313ce4401>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m                   \u001b[0mmin_buffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                   handle_ep=_handle_ep)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-75cace16d1f8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_steps, player, replay_buffer, optimize_op, train_interval, target_interval, batch_size, min_buffer_size, tf_schedules, handle_ep, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_taken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msteps_taken\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnext_target_update\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3ea2ce21c9fb>\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(self, samples, new_weights)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3ea2ce21c9fb>\u001b[0m in \u001b[0;36mset_value\u001b[0;34m(self, idx, value)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;34m\"\"\"Set the value at the given index.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capacity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3ea2ce21c9fb>\u001b[0m in \u001b[0;36m_set_idx\u001b[0;34m(self, idx, value)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mneeds_recompute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dp3iWaYcDZc",
        "colab_type": "code",
        "outputId": "6c113e7f-26d8-4ec0-f9ad-218a84df3dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-14 12:31:24--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.206.126.139, 3.229.196.117, 52.73.147.107, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.206.126.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  14.4MB/s    in 0.9s    \n",
            "\n",
            "2019-12-14 12:31:25 (14.4 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v-GqeiDcFLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = '/content/drive/My Drive/ANresults'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4TZSrYHcvhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3punndnKcyF0",
        "colab_type": "code",
        "outputId": "13c66de0-99d0-4d01-e780-abd182ef1ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://fc05fe3e.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7jS6xaaj4Er",
        "colab_type": "code",
        "outputId": "396cf968-34fd-4bc1-ac40-aa9c78be7d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!tensorboard --logdir='/content/drive/My Drive/ANresults' "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorBoard 1.15.0 at http://482ce1a6b003:6006/ (Press CTRL+C to quit)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}