{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anyrl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyHaski/DL_AtariRainbow/blob/master/Anyrl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GzKRIRNUhzh",
        "colab_type": "code",
        "outputId": "b3c4e3f1-95de-4ce5-b28a-907287e6cc80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsT1hn__UWop",
        "colab_type": "code",
        "outputId": "e5ccc957-70ab-453c-9eac-a026f4cb8b12",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        }
      },
      "source": [
        "#@title Install Packages\n",
        "!pip install anyrl "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting anyrl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/07/8924a62b35dfb681d88c148de055059ccc8adf1759551281aae733bdfb64/anyrl-0.12.23-py3-none-any.whl (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from anyrl) (1.2.2)\n",
            "Collecting pandas<0.24.0,>=0.20.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 15.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from anyrl) (1.17.4)\n",
            "Collecting gym<0.11.0,>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/04/70d4901b7105082c9742acd64728342f6da7cd471572fd0660a73f9cfe27/gym-0.10.11.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas<0.24.0,>=0.20.0->anyrl) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas<0.24.0,>=0.20.0->anyrl) (2.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym<0.11.0,>=0.9.6->anyrl) (1.3.3)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.11.0,>=0.9.6->anyrl) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<0.11.0,>=0.9.6->anyrl) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.11.0,>=0.9.6->anyrl) (1.3.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<0.11.0,>=0.9.6->anyrl) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<0.11.0,>=0.9.6->anyrl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<0.11.0,>=0.9.6->anyrl) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<0.11.0,>=0.9.6->anyrl) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym<0.11.0,>=0.9.6->anyrl) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.11-cp36-none-any.whl size=1588314 sha256=1b8d9111bab67b129ef7edc90c4551973bfdc30dc7c64559e2fbf4e047dd7b31\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/eb/1f/22c4124f3c64943aa0646daf4612b1c1f00f27d89b81304ebd\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.25.0; python_version >= \"3.0\", but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pandas, gym, anyrl\n",
            "  Found existing installation: pandas 0.25.3\n",
            "    Uninstalling pandas-0.25.3:\n",
            "      Successfully uninstalled pandas-0.25.3\n",
            "  Found existing installation: gym 0.15.4\n",
            "    Uninstalling gym-0.15.4:\n",
            "      Successfully uninstalled gym-0.15.4\n",
            "Successfully installed anyrl-0.12.23 gym-0.10.11 pandas-0.23.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kswz0Ca1Bi20",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Parameters\n",
        "RUN='5'\n",
        "RUN_RESTORE='5_3'\n",
        "RUN_NUM='5_4'\n",
        "\n",
        "\"\"\"\n",
        "Hyperparameters\n",
        "\n",
        "\"\"\"\n",
        "workers=8\n",
        "lr=1e-4\n",
        "#lr_epsilon=1.5e-4\n",
        "num_atoms=51\n",
        "v_min=-10\n",
        "v_max=10\n",
        "#noisy_sigma=0.5\n",
        "n_step=3\n",
        "#dueling=True\n",
        "min_buffer_size=20000\n",
        "num_steps=2000000\n",
        "buffer_size=500000\n",
        "target_interval=8192\n",
        "batch_size=32\n",
        "replay_epsilon=0.1\n",
        "replay_alpha=0.5\n",
        "replay_beta=0.4\n",
        "game='SpaceInvaders'\n",
        "restore=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGFWQ9KT27u9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Modified Replay Buffer Code\n",
        "\"\"\"\n",
        "Various replay buffer implementations.\n",
        "\"\"\"\n",
        "\n",
        "from math import sqrt\n",
        "import random\n",
        "import pickle\n",
        "from anyrl.rollouts import ReplayBuffer\n",
        "import numpy as np\n",
        "\n",
        "class ModifiedPrioritizedReplayBuffer(ReplayBuffer):\n",
        "    \"\"\"\n",
        "    A prioritized replay buffer with loss-proportional\n",
        "    sampling.\n",
        "    Weights passed to add_sample() and update_weights()\n",
        "    are assumed to be error terms (e.g. the absolute TD\n",
        "    error).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity, alpha, beta, first_max=1, epsilon=0):\n",
        "        \"\"\"\n",
        "        Create a prioritized replay buffer.\n",
        "        The beta parameter can be any object that has\n",
        "        support for the float() built-in.\n",
        "        This way, you can use a TFScheduleValue.\n",
        "        Args:\n",
        "          capacity: the maximum number of transitions to\n",
        "            store in the buffer.\n",
        "          alpha: an exponent controlling the temperature.\n",
        "            Higher values result in more prioritization.\n",
        "            A value of 0 yields uniform prioritization.\n",
        "          beta: an exponent controlling the amount of\n",
        "            importance sampling. A value of 1 yields\n",
        "            unbiased sampling. A value of 0 yields no\n",
        "            importance sampling.\n",
        "          first_max: the initial weight for new samples\n",
        "            when no init_weight is specified and the\n",
        "            buffer is completely empty.\n",
        "          epsilon: a value which is added to every error\n",
        "            term before the error term is used.\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.transitions = []\n",
        "        self.errors = FloatBuffer(capacity)\n",
        "        self._max_weight_arg = first_max\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return len(self.transitions)\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        indices, probs = self.errors.sample(num_samples)\n",
        "        beta = float(self.beta)\n",
        "        importance_weights = np.power(probs * self.size, -beta)\n",
        "        importance_weights /= np.power(self.errors.min() / self.errors.sum() * self.size, -beta)\n",
        "        samples = []\n",
        "        for i, weight in zip(indices, importance_weights):\n",
        "            sample = self.transitions[i].copy()\n",
        "            sample['weight'] = weight\n",
        "            sample['id'] = i\n",
        "            samples.append(sample)\n",
        "        return samples\n",
        "\n",
        "    def add_sample(self, sample, init_weight=None):\n",
        "        \"\"\"\n",
        "        Add a sample to the buffer.\n",
        "        When new samples are added without an explicit\n",
        "        initial weight, the maximum weight argument ever\n",
        "        seen is used. When the buffer is empty, first_max\n",
        "        is used.\n",
        "        \"\"\"\n",
        "        self.transitions.append(sample)\n",
        "        if init_weight is None:\n",
        "            self.errors.append(self._process_weight(self._max_weight_arg))\n",
        "        else:\n",
        "            self.errors.append(self._process_weight(init_weight))\n",
        "        while len(self.transitions) > self.capacity:\n",
        "            del self.transitions[0]\n",
        "\n",
        "    def update_weights(self, samples, new_weights):\n",
        "        for sample, weight in zip(samples, new_weights):\n",
        "            self.errors.set_value(sample['id'], self._process_weight(weight))\n",
        "\n",
        "    def _process_weight(self, weight):\n",
        "        self._max_weight_arg = max(self._max_weight_arg, weight)\n",
        "        return (weight + self.epsilon) ** self.alpha\n",
        "\n",
        "    #saving replay buffer to pickle file\n",
        "    def save_samples(self):\n",
        "        print(\"saving transitions: \",len(self.transitions))\n",
        "        with open('/content/drive/My Drive/AN/transitions'+RUN_NUM+'.p','wb') as handle:\n",
        "            pickle.dump(self.transitions,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    #loading replay buffer from pickle file\n",
        "        #have to add sample by sample due to populating the errors into the FloatBuffer (self.errors)\n",
        "    def load_samples(self):\n",
        "        temp_trans = pickle.load(open(\"/content/drive/My Drive/AN/transitions\"+RUN_RESTORE+\".p\",\"rb\"))\n",
        "        for i in temp_trans:\n",
        "            self.add_sample(i)\n",
        "        print(\"loaded transitions: \",len(self.transitions))\n",
        "\n",
        "class FloatBuffer:\n",
        "    \"\"\"A ring-buffer of floating point values.\"\"\"\n",
        "\n",
        "    def __init__(self, capacity, dtype='float64'):\n",
        "        self._capacity = capacity\n",
        "        self._start = 0\n",
        "        self._used = 0\n",
        "        self._buffer = np.zeros((capacity,), dtype=dtype)\n",
        "        self._bin_size = int(sqrt(capacity))\n",
        "        num_bins = capacity // self._bin_size\n",
        "        if num_bins * self._bin_size < capacity:\n",
        "            num_bins += 1\n",
        "        self._bin_sums = np.zeros((num_bins,), dtype=dtype)\n",
        "        self._min = 0\n",
        "\n",
        "    def append(self, value):\n",
        "        \"\"\"\n",
        "        Add a value to the end of the buffer.\n",
        "        If the buffer is full, the first value is removed.\n",
        "        \"\"\"\n",
        "        idx = (self._start + self._used) % self._capacity\n",
        "        if self._used < self._capacity:\n",
        "            self._used += 1\n",
        "        else:\n",
        "            self._start = (self._start + 1) % self._capacity\n",
        "        self._set_idx(idx, value)\n",
        "\n",
        "    def sample(self, num_values):\n",
        "        \"\"\"\n",
        "        Sample indices in proportion to their value.\n",
        "        Returns:\n",
        "          A tuple (indices, probs)\n",
        "        \"\"\"\n",
        "        assert self._used >= num_values\n",
        "        res = []\n",
        "        probs = []\n",
        "        bin_probs = self._bin_sums / np.sum(self._bin_sums)\n",
        "        while len(res) < num_values:\n",
        "            bin_idx = np.random.choice(len(self._bin_sums), p=bin_probs)\n",
        "            bin_values = self._bin(bin_idx)\n",
        "            sub_probs = bin_values / np.sum(bin_values)\n",
        "            sub_idx = np.random.choice(len(bin_values), p=sub_probs)\n",
        "            idx = bin_idx * self._bin_size + sub_idx\n",
        "            res.append(idx)\n",
        "            probs.append(bin_probs[bin_idx] * sub_probs[sub_idx])\n",
        "        return (np.array(list(res)) - self._start) % self._capacity, np.array(probs)\n",
        "\n",
        "    def set_value(self, idx, value):\n",
        "        \"\"\"Set the value at the given index.\"\"\"\n",
        "        idx = (idx + self._start) % self._capacity\n",
        "        self._set_idx(idx, value)\n",
        "\n",
        "    def min(self):\n",
        "        \"\"\"Get the minimum value in the buffer.\"\"\"\n",
        "        return self._min\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Get the sum of the values in the buffer.\"\"\"\n",
        "        return np.sum(self._bin_sums)\n",
        "\n",
        "    def _set_idx(self, idx, value):\n",
        "        assert not np.isnan(value)\n",
        "        assert value > 0\n",
        "        needs_recompute = False\n",
        "        if self._min == self._buffer[idx]:\n",
        "            needs_recompute = True\n",
        "        elif value < self._min:\n",
        "            self._min = value\n",
        "        bin_idx = idx // self._bin_size\n",
        "        self._buffer[idx] = value\n",
        "        self._bin_sums[bin_idx] = np.sum(self._bin(bin_idx))\n",
        "        if needs_recompute:\n",
        "            self._recompute_min()\n",
        "\n",
        "    def _bin(self, bin_idx):\n",
        "        if bin_idx == len(self._bin_sums) - 1:\n",
        "            return self._buffer[self._bin_size * bin_idx:]\n",
        "        return self._buffer[self._bin_size * bin_idx:self._bin_size * (bin_idx + 1)]\n",
        "\n",
        "    def _recompute_min(self):\n",
        "        if self._used < self._capacity:\n",
        "            self._min = np.min(self._buffer[:self._used])\n",
        "        else:\n",
        "            self._min = np.min(self._buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q28xoaCVBFG_",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Modified DQN Code\n",
        "%tensorflow_version 1.x\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class DQN:\n",
        "    \"\"\"\n",
        "    Train TFQNetwork models using Q-learning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, online_net, target_net, discount=0.99):\n",
        "        \"\"\"\n",
        "        Create a Q-learning session.\n",
        "        Args:\n",
        "          online_net: the online TFQNetwork.\n",
        "          target_net: the target TFQNetwork.\n",
        "          discount: the per-step discount factor.\n",
        "        \"\"\"\n",
        "        self.online_net = online_net\n",
        "        self.target_net = target_net\n",
        "        self.discount = discount\n",
        "\n",
        "        obs_shape = (None,) + online_net.obs_vectorizer.out_shape\n",
        "        self.obses_ph = tf.placeholder(online_net.input_dtype, shape=obs_shape)\n",
        "        self.actions_ph = tf.placeholder(tf.int32, shape=(None,))\n",
        "        self.rews_ph = tf.placeholder(tf.float32, shape=(None,))\n",
        "        self.new_obses_ph = tf.placeholder(online_net.input_dtype, shape=obs_shape)\n",
        "        self.terminals_ph = tf.placeholder(tf.bool, shape=(None,))\n",
        "        self.discounts_ph = tf.placeholder(tf.float32, shape=(None,))\n",
        "        self.weights_ph = tf.placeholder(tf.float32, shape=(None,))\n",
        "\n",
        "        losses = online_net.transition_loss(target_net, self.obses_ph, self.actions_ph,\n",
        "                                            self.rews_ph, self.new_obses_ph, self.terminals_ph,\n",
        "                                            self.discounts_ph)\n",
        "        self.losses = self.weights_ph * losses\n",
        "        self.loss = tf.reduce_mean(self.losses)\n",
        "\n",
        "        assigns = []\n",
        "        for dst, src in zip(target_net.variables, online_net.variables):\n",
        "            assigns.append(tf.assign(dst, src))\n",
        "        self.update_target = tf.group(*assigns)\n",
        "\n",
        "    def feed_dict(self, transitions):\n",
        "        \"\"\"\n",
        "        Generate a feed_dict that feeds the batch of\n",
        "        transitions to the DQN loss terms.\n",
        "        Args:\n",
        "          transition: a sequence of transition dicts, as\n",
        "            defined in anyrl.rollouts.ReplayBuffer.\n",
        "        Returns:\n",
        "          A dict which can be fed to tf.Session.run().\n",
        "        \"\"\"\n",
        "        obs_vect = self.online_net.obs_vectorizer\n",
        "        res = {\n",
        "            self.obses_ph: obs_vect.to_vecs([t['obs'] for t in transitions]),\n",
        "            self.actions_ph: [t['model_outs']['actions'][0] for t in transitions],\n",
        "            self.rews_ph: [self._discounted_rewards(t['rewards']) for t in transitions],\n",
        "            self.terminals_ph: [t['new_obs'] is None for t in transitions],\n",
        "            self.discounts_ph: [(self.discount ** len(t['rewards'])) for t in transitions],\n",
        "            self.weights_ph: [t['weight'] for t in transitions]\n",
        "        }\n",
        "        new_obses = []\n",
        "        for trans in transitions:\n",
        "            if trans['new_obs'] is None:\n",
        "                new_obses.append(trans['obs'])\n",
        "            else:\n",
        "                new_obses.append(trans['new_obs'])\n",
        "        res[self.new_obses_ph] = obs_vect.to_vecs(new_obses)\n",
        "        return res\n",
        "\n",
        "    def optimize(self, learning_rate=6.25e-5, epsilon=1.5e-4, **adam_kwargs):\n",
        "        \"\"\"\n",
        "        Create a TF Op that optimizes the objective.\n",
        "        Args:\n",
        "          learning_rate: the Adam learning rate.\n",
        "          epsilon: the Adam epsilon.\n",
        "        \"\"\"\n",
        "        optim = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon, **adam_kwargs)\n",
        "        return optim.minimize(self.loss)\n",
        "\n",
        "    def train(self,\n",
        "              num_steps,\n",
        "              player,\n",
        "              replay_buffer,\n",
        "              optimize_op,\n",
        "              train_interval=1,\n",
        "              target_interval=8192,\n",
        "              batch_size=32,\n",
        "              min_buffer_size=20000,\n",
        "              tf_schedules=(),\n",
        "              handle_ep=lambda steps, rew: None,\n",
        "              timeout=None):\n",
        "        \"\"\"\n",
        "        Run an automated training loop.\n",
        "        This is meant to provide a convenient way to run a\n",
        "        standard training loop without any modifications.\n",
        "        You may get more flexibility by writing your own\n",
        "        training loop.\n",
        "        Args:\n",
        "          num_steps: the number of timesteps to run.\n",
        "          player: the Player for gathering experience.\n",
        "          replay_buffer: the ReplayBuffer for experience.\n",
        "          optimize_op: a TF Op to optimize the model.\n",
        "          train_interval: timesteps per training step.\n",
        "          target_interval: number of timesteps between\n",
        "            target network updates.\n",
        "          batch_size: the size of experience mini-batches.\n",
        "          min_buffer_size: minimum replay buffer size\n",
        "            before training is performed.\n",
        "          tf_schedules: a sequence of TFSchedules that are\n",
        "            updated with the number of steps taken.\n",
        "          handle_ep: called with information about every\n",
        "            completed episode.\n",
        "          timeout: if set, this is a number of seconds\n",
        "            after which the training loop should exit.\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        sess = self.online_net.session\n",
        "        \n",
        "        tnrewsph = tf.summary.scalar(name='rews_ph', tensor=tf.reduce_mean(self.rews_ph))\n",
        "        tndiscountsph = tf.summary.scalar(name='discounts_ph', tensor=tf.reduce_mean(self.discounts_ph))\n",
        "        tnweightsph = tf.summary.scalar(name='weights_ph', tensor=tf.reduce_mean(self.weights_ph))\n",
        "        tnlosses = tf.summary.scalar(name='losses', tensor=tf.reduce_mean(self.losses))\n",
        "\n",
        "        merge = tf.summary.merge([tnrewsph,tndiscountsph,tnweightsph,tnlosses])\n",
        "        train_writer = tf.summary.FileWriter( '/content/drive/My Drive/AN/logs/'+ RUN_NUM +'/train', sess.graph)\n",
        "        \n",
        "        sess.run(self.update_target)\n",
        "        steps_taken = 0\n",
        "        next_target_update = target_interval\n",
        "        next_train_step = train_interval\n",
        "        start_time = time.time()\n",
        "\n",
        "        if restore:\n",
        "          replay_buffer.load_samples()\n",
        "\n",
        "        while steps_taken < num_steps:\n",
        "            if timeout is not None and time.time() - start_time > timeout:\n",
        "                return\n",
        "            transitions = player.play()\n",
        "            for trans in transitions:\n",
        "                if trans['is_last']:\n",
        "                    handle_ep(trans['episode_step'] + 1, trans['total_reward'], trans['episode_id'])\n",
        "                replay_buffer.add_sample(trans)\n",
        "                steps_taken += 1\n",
        "                for sched in tf_schedules:\n",
        "                    sched.add_time(sess, 1)\n",
        "                if replay_buffer.size >= min_buffer_size and steps_taken >= next_train_step:\n",
        "                    next_train_step = steps_taken + train_interval\n",
        "                    batch = replay_buffer.sample(batch_size)\n",
        "                    \n",
        "                    #_, losses = sess.run((optimize_op, self.losses),\n",
        "                    #                     feed_dict=self.feed_dict(batch))\n",
        "                    _, losses, summary = sess.run((optimize_op, self.losses, merge),\n",
        "                                         feed_dict=self.feed_dict(batch))\n",
        "                    \n",
        "                    train_writer.add_summary(summary, steps_taken)\n",
        "                    replay_buffer.update_weights(batch, losses)\n",
        "                    \n",
        "                if steps_taken >= next_target_update:\n",
        "                    next_target_update = steps_taken + target_interval\n",
        "                    sess.run(self.update_target)\n",
        "                if (steps_taken % 100000 ==0):\n",
        "                   replay_buffer.save_samples()\n",
        "        replay_buffer.save_samples()\n",
        "\n",
        "    def _discounted_rewards(self, rews):\n",
        "        res = 0\n",
        "        for i, rew in enumerate(rews):\n",
        "            res += rew * (self.discount ** i)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37Krqkiff8Sc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title para agrs don't work \n",
        "import argparse\n",
        "\n",
        "def _parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--lr', help='Adam learning rate', type=float, default=6.25e-5)\n",
        "    parser.add_argument('--lr-epsilon', help='Adam epsilon', type=float, default=1.5e-4)\n",
        "    parser.add_argument('--num-atoms', help='Number of distribution atoms', type=int, default=51)\n",
        "    parser.add_argument('--v-min', help='Minimum atom value', type=int, default=-10)\n",
        "    parser.add_argument('--v-max', help='Maximum atom value', type=int, default=10)\n",
        "    parser.add_argument('--noisy-sigma', help='initial Noisy Net noise', type=float, default=0.5)\n",
        "    parser.add_argument('--n-step', help='Number of multi-step', type=int, default=3)\n",
        "    #parser.add_argument('--dueling', help='Set dueling architecture', type=bool, default=True)\n",
        "    parser.add_argument('--min-buffer-size', help='replay buffer size before training',\n",
        "                        type=int, default=20000)\n",
        "    parser.add_argument('--num-steps', help='Number of max steps to train', type=int, default=2000000)\n",
        "    parser.add_argument('--buffer-size', help='replay buffer size', type=int, default=1000000)\n",
        "   #parser.add_argument('--workers', help='number of parallel envs', type=int, default=8)\n",
        "    parser.add_argument('--target-interval', help='training iters per log', type=int, default=8192)\n",
        "    parser.add_argument('--batch-size', help='SGD batch size', type=int, default=32)\n",
        "    parser.add_argument('--replay-epsilon', help='initial epsilon', type=float, default=0.0)\n",
        "    parser.add_argument('--replay-alpha', help='Replay alpha', type=float, default=0.5)\n",
        "    parser.add_argument('--replay-beta', help='Replay beta', type=float, default=0.4)\n",
        "    parser.add_argument('game', help='game name', default='SpaceInvaders')\n",
        "    parser.add_argument('--restore', help='load from checkpoint', action='store_true')\n",
        "    return parser.parse_args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vq_lTHZV3iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afc4j-0zMTC2",
        "colab_type": "code",
        "outputId": "c97567e7-08ec-4cd0-cbc4-db718cbe1b1a",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title DQN Train\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "from gym.wrappers import Monitor\n",
        "from functools import partial\n",
        "from anyrl.envs import batched_gym_env\n",
        "from anyrl.envs.wrappers import BatchedFrameStack, DownsampleEnv, GrayscaleEnv\n",
        "from anyrl.models import rainbow_models\n",
        "from anyrl.rollouts import BatchedPlayer, NStepPlayer\n",
        "from anyrl.spaces import gym_space_vectorizer\n",
        "from anyrl.utils import tf_state\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "REWARD_HISTORY = 10\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, '/content/drive/My Drive/videoAnyrl/'+RUN_NUM, force=True )\n",
        "  return env\n",
        "\n",
        "def make_env():\n",
        "    \"\"\"\n",
        "    Create an environment with some standard wrappers.\n",
        "    \"\"\"\n",
        "    #env = retro.make(game='SpaceInvaders-Atari2600', record=True)\n",
        "    env = wrap_env(gym.make(game+'-v0'))\n",
        "    return GrayscaleEnv(DownsampleEnv(env, 2))\n",
        "\n",
        "def main():\n",
        "    env = batched_gym_env([partial(make_env)]* workers)\n",
        "    env = BatchedFrameStack(env, num_images=4, concat=False)\n",
        "\n",
        "    checkpoint_dir = os.path.join(os.getcwd(), 'drive/My Drive/ANresults')\n",
        "    results_dir = os.path.join(os.getcwd(), 'drive/My Drive/ANresults', time.strftime(\"%d-%m-%Y_%H-%M-%S\")+ RUN_NUM)\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "    summary_writer = tf.summary.FileWriter(results_dir)\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True # pylint: disable=E1101\n",
        "    \n",
        "    \n",
        "    with tf.Session(config=config) as sess:  \n",
        "        \n",
        "        dqn=DQN(*rainbow_models(sess,\n",
        "                              env.action_space.n,\n",
        "                              gym_space_vectorizer(env.observation_space),\n",
        "                              num_atoms=num_atoms,\n",
        "                              min_val=v_min,\n",
        "                              max_val=v_max))\n",
        "\n",
        "        player = NStepPlayer(BatchedPlayer(env, dqn.online_net), n_step)\n",
        "        optimize = dqn.optimize(learning_rate=lr)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        if(restore):\n",
        "          tf_state.load_vars(sess,'/content/drive/My Drive/AN/anyrlM'+RUN_RESTORE)\n",
        "\n",
        "        reward_hist = []\n",
        "        total_steps = 0\n",
        "        save_model_steps=100000\n",
        "        episodes=0\n",
        "\n",
        "        def _handle_ep(steps, rew, id):\n",
        "            nonlocal total_steps\n",
        "            nonlocal episodes\n",
        "            nonlocal save_model_steps\n",
        "            total_steps += steps\n",
        "            episodes += 1\n",
        "            reward_hist.append(rew)\n",
        "\n",
        "            summary_reward = tf.Summary()\n",
        "            summary_reward.value.add(tag='global/reward', simple_value=rew)\n",
        "            summary_writer.add_summary(summary_reward, global_step=total_steps)\n",
        "\n",
        "            if len(reward_hist) == REWARD_HISTORY:\n",
        "              print('ID: %d | %d steps | %f mean' % (id,total_steps, sum(reward_hist) / len(reward_hist)))\n",
        "            \n",
        "              summary_meanreward = tf.Summary()\n",
        "              summary_meanreward.value.add(tag='global/mean_reward', simple_value=sum(reward_hist) / len(reward_hist))\n",
        "              summary_writer.add_summary(summary_meanreward, global_step=total_steps)\n",
        "\n",
        "              reward_hist.clear()\n",
        "            if(total_steps>= save_model_steps):\n",
        "              save_model_steps +=100000\n",
        "              print('save model')\n",
        "              tf_state.save_vars(sess,'/content/drive/My Drive/AN/anyrlM'+RUN_NUM)\n",
        "                 \n",
        "        \n",
        "        dqn.train(num_steps=num_steps, \n",
        "                  player=player,\n",
        "                  replay_buffer=ModifiedPrioritizedReplayBuffer(buffer_size, replay_alpha, replay_beta, epsilon=replay_epsilon),\n",
        "                  optimize_op=optimize,\n",
        "                  train_interval=1,\n",
        "                  target_interval=target_interval,\n",
        "                  batch_size=batch_size,\n",
        "                  min_buffer_size=min_buffer_size,\n",
        "                  handle_ep=_handle_ep)\n",
        "        \n",
        "        print('save model')\n",
        "        tf_state.save_vars(sess,'/content/drive/My Drive/AN/anyrlM'+RUN_NUM)\n",
        "    env.close()\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "      main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:78: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:79: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:80: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/util.py:80: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_scalar.py:247: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_scalar.py:260: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:113: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:273: The name tf.ceil is deprecated. Please use tf.math.ceil instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/anyrl/models/dqn_dist.py:289: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Loading variables from /content/drive/My Drive/AN/anyrlM5_3 ...\n",
            "loaded transitions:  500000\n",
            "ID: 10 | 8101 steps | 284.000000 mean\n",
            "ID: 19 | 15709 steps | 293.000000 mean\n",
            "ID: 30 | 23990 steps | 348.000000 mean\n",
            "ID: 40 | 32417 steps | 278.500000 mean\n",
            "ID: 49 | 40506 steps | 331.000000 mean\n",
            "ID: 59 | 48498 steps | 262.000000 mean\n",
            "ID: 69 | 56981 steps | 390.000000 mean\n",
            "ID: 80 | 64987 steps | 288.500000 mean\n",
            "ID: 88 | 73516 steps | 326.000000 mean\n",
            "ID: 100 | 82657 steps | 306.500000 mean\n",
            "ID: 109 | 91381 steps | 307.500000 mean\n",
            "saving transitions:  500000\n",
            "ID: 118 | 98358 steps | 251.000000 mean\n",
            "save model\n",
            "ID: 126 | 105991 steps | 278.000000 mean\n",
            "ID: 135 | 113942 steps | 275.000000 mean\n",
            "ID: 149 | 121617 steps | 248.500000 mean\n",
            "ID: 160 | 128632 steps | 249.000000 mean\n",
            "ID: 169 | 136706 steps | 269.500000 mean\n",
            "ID: 182 | 144425 steps | 284.000000 mean\n",
            "ID: 186 | 152536 steps | 289.000000 mean\n",
            "ID: 199 | 160657 steps | 297.000000 mean\n",
            "ID: 212 | 168301 steps | 251.500000 mean\n",
            "ID: 218 | 176379 steps | 293.500000 mean\n",
            "ID: 231 | 183994 steps | 216.000000 mean\n",
            "ID: 239 | 191931 steps | 254.000000 mean\n",
            "saving transitions:  500000\n",
            "ID: 247 | 198249 steps | 215.000000 mean\n",
            "save model\n",
            "ID: 260 | 206058 steps | 300.500000 mean\n",
            "ID: 268 | 213281 steps | 187.000000 mean\n",
            "ID: 280 | 219212 steps | 169.500000 mean\n",
            "ID: 294 | 227256 steps | 308.500000 mean\n",
            "ID: 300 | 234626 steps | 251.500000 mean\n",
            "ID: 304 | 243523 steps | 329.500000 mean\n",
            "ID: 322 | 252759 steps | 301.000000 mean\n",
            "ID: 328 | 259293 steps | 235.000000 mean\n",
            "ID: 339 | 266723 steps | 266.500000 mean\n",
            "ID: 350 | 274939 steps | 290.500000 mean\n",
            "ID: 359 | 284889 steps | 341.000000 mean\n",
            "ID: 365 | 292567 steps | 270.500000 mean\n",
            "saving transitions:  500000\n",
            "save model\n",
            "ID: 379 | 301556 steps | 342.500000 mean\n",
            "ID: 388 | 310069 steps | 284.000000 mean\n",
            "ID: 396 | 316431 steps | 190.500000 mean\n",
            "ID: 407 | 324210 steps | 285.000000 mean\n",
            "ID: 419 | 332301 steps | 283.500000 mean\n",
            "ID: 433 | 339258 steps | 234.000000 mean\n",
            "ID: 438 | 348052 steps | 361.000000 mean\n",
            "ID: 450 | 356407 steps | 287.000000 mean\n",
            "ID: 461 | 363512 steps | 257.000000 mean\n",
            "ID: 469 | 371831 steps | 316.000000 mean\n",
            "ID: 479 | 379333 steps | 221.000000 mean\n",
            "ID: 486 | 388212 steps | 292.500000 mean\n",
            "ID: 503 | 394621 steps | 198.000000 mean\n",
            "saving transitions:  500000\n",
            "save model\n",
            "ID: 516 | 402845 steps | 288.000000 mean\n",
            "ID: 519 | 411599 steps | 344.000000 mean\n",
            "ID: 531 | 418283 steps | 215.000000 mean\n",
            "ID: 541 | 426396 steps | 323.000000 mean\n",
            "ID: 551 | 434158 steps | 242.000000 mean\n",
            "ID: 564 | 443465 steps | 360.000000 mean\n",
            "ID: 568 | 452244 steps | 325.500000 mean\n",
            "ID: 578 | 461011 steps | 328.000000 mean\n",
            "ID: 588 | 469271 steps | 304.000000 mean\n",
            "ID: 600 | 477615 steps | 351.000000 mean\n",
            "ID: 606 | 485397 steps | 269.500000 mean\n",
            "ID: 624 | 492190 steps | 256.000000 mean\n",
            "saving transitions:  500000\n",
            "ID: 630 | 499739 steps | 251.500000 mean\n",
            "save model\n",
            "ID: 635 | 508450 steps | 317.500000 mean\n",
            "ID: 649 | 516243 steps | 254.000000 mean\n",
            "ID: 657 | 524820 steps | 297.500000 mean\n",
            "ID: 671 | 532020 steps | 267.000000 mean\n",
            "ID: 681 | 539416 steps | 261.500000 mean\n",
            "ID: 686 | 549509 steps | 384.500000 mean\n",
            "ID: 701 | 556909 steps | 241.500000 mean\n",
            "ID: 712 | 565261 steps | 268.000000 mean\n",
            "ID: 717 | 571862 steps | 211.500000 mean\n",
            "ID: 729 | 579763 steps | 254.500000 mean\n",
            "ID: 742 | 586625 steps | 231.500000 mean\n",
            "ID: 749 | 594419 steps | 249.500000 mean\n",
            "saving transitions:  500000\n",
            "save model\n",
            "ID: 761 | 601600 steps | 210.500000 mean\n",
            "ID: 771 | 610278 steps | 306.000000 mean\n",
            "ID: 779 | 618639 steps | 276.000000 mean\n",
            "ID: 788 | 626083 steps | 260.000000 mean\n",
            "ID: 802 | 633773 steps | 298.500000 mean\n",
            "ID: 812 | 640904 steps | 246.500000 mean\n",
            "ID: 818 | 650249 steps | 326.500000 mean\n",
            "ID: 827 | 658161 steps | 277.000000 mean\n",
            "ID: 843 | 665995 steps | 289.000000 mean\n",
            "ID: 848 | 674828 steps | 318.000000 mean\n",
            "ID: 861 | 682576 steps | 310.500000 mean\n",
            "ID: 867 | 690958 steps | 300.500000 mean\n",
            "saving transitions:  500000\n",
            "ID: 879 | 698638 steps | 270.000000 mean\n",
            "save model\n",
            "ID: 890 | 707298 steps | 276.000000 mean\n",
            "ID: 899 | 714693 steps | 255.000000 mean\n",
            "ID: 911 | 721562 steps | 262.500000 mean\n",
            "ID: 921 | 728694 steps | 271.000000 mean\n",
            "ID: 930 | 736201 steps | 244.000000 mean\n",
            "ID: 939 | 743599 steps | 271.500000 mean\n",
            "ID: 945 | 751653 steps | 263.500000 mean\n",
            "ID: 957 | 759628 steps | 307.500000 mean\n",
            "ID: 970 | 767337 steps | 294.000000 mean\n",
            "ID: 982 | 775096 steps | 304.500000 mean\n",
            "ID: 991 | 782443 steps | 262.500000 mean\n",
            "ID: 1000 | 791098 steps | 318.000000 mean\n",
            "saving transitions:  500000\n",
            "ID: 1010 | 798456 steps | 238.500000 mean\n",
            "save model\n",
            "ID: 1021 | 804670 steps | 188.500000 mean\n",
            "ID: 1028 | 811898 steps | 226.000000 mean\n",
            "ID: 1040 | 819916 steps | 271.500000 mean\n",
            "ID: 1051 | 827779 steps | 237.000000 mean\n",
            "ID: 1056 | 836016 steps | 297.500000 mean\n",
            "ID: 1069 | 843148 steps | 250.500000 mean\n",
            "ID: 1080 | 849392 steps | 197.500000 mean\n",
            "ID: 1089 | 857100 steps | 287.000000 mean\n",
            "ID: 1098 | 866294 steps | 312.500000 mean\n",
            "ID: 1112 | 873790 steps | 280.500000 mean\n",
            "ID: 1121 | 880855 steps | 271.500000 mean\n",
            "ID: 1129 | 889797 steps | 321.000000 mean\n",
            "ID: 1137 | 897904 steps | 256.500000 mean\n",
            "saving transitions:  500000\n",
            "save model\n",
            "ID: 1146 | 905569 steps | 291.500000 mean\n",
            "ID: 1160 | 912671 steps | 229.000000 mean\n",
            "ID: 1172 | 920346 steps | 294.000000 mean\n",
            "ID: 1173 | 929223 steps | 378.000000 mean\n",
            "ID: 1189 | 938089 steps | 335.500000 mean\n",
            "ID: 1199 | 945266 steps | 284.500000 mean\n",
            "ID: 1205 | 953104 steps | 284.500000 mean\n",
            "ID: 1216 | 960580 steps | 229.000000 mean\n",
            "ID: 1230 | 967582 steps | 230.500000 mean\n",
            "ID: 1244 | 974097 steps | 217.500000 mean\n",
            "ID: 1254 | 981740 steps | 267.500000 mean\n",
            "ID: 1260 | 990219 steps | 273.500000 mean\n",
            "saving transitions:  500000\n",
            "ID: 1266 | 998524 steps | 310.000000 mean\n",
            "save model\n",
            "ID: 1279 | 1005735 steps | 234.000000 mean\n",
            "ID: 1291 | 1013151 steps | 244.500000 mean\n",
            "ID: 1300 | 1021251 steps | 324.000000 mean\n",
            "ID: 1307 | 1028644 steps | 242.000000 mean\n",
            "ID: 1321 | 1035094 steps | 237.000000 mean\n",
            "ID: 1325 | 1043617 steps | 271.000000 mean\n",
            "ID: 1339 | 1050845 steps | 228.000000 mean\n",
            "ID: 1349 | 1058939 steps | 293.000000 mean\n",
            "ID: 1356 | 1066350 steps | 286.000000 mean\n",
            "ID: 1369 | 1073974 steps | 277.500000 mean\n",
            "ID: 1382 | 1082142 steps | 309.000000 mean\n",
            "ID: 1390 | 1090216 steps | 284.000000 mean\n",
            "saving transitions:  500000\n",
            "ID: 1399 | 1098813 steps | 333.000000 mean\n",
            "save model\n",
            "ID: 1408 | 1106979 steps | 278.000000 mean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dp3iWaYcDZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v-GqeiDcFLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = '/content/drive/My Drive/AN/logs/5_1/train'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4TZSrYHcvhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3punndnKcyF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7jS6xaaj4Er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard --logdir='/content/drive/My Drive/AN/logs/5_1/train' "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}